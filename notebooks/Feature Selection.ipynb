{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso,LassoLarsCV, LassoLars, ElasticNetCV, ElasticNet\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"PGT121_Neu_OccAA.csv\", header =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>IC50_mean</th>\n",
       "      <th>Clade</th>\n",
       "      <th>Name</th>\n",
       "      <th>Accession</th>\n",
       "      <th>Pos1_-</th>\n",
       "      <th>Pos1_W</th>\n",
       "      <th>Pos1_F</th>\n",
       "      <th>Pos1_G</th>\n",
       "      <th>Pos1_A</th>\n",
       "      <th>...</th>\n",
       "      <th>N611</th>\n",
       "      <th>N615_1</th>\n",
       "      <th>N616</th>\n",
       "      <th>N618</th>\n",
       "      <th>N625</th>\n",
       "      <th>N637</th>\n",
       "      <th>N674</th>\n",
       "      <th>N743</th>\n",
       "      <th>N816</th>\n",
       "      <th>N824</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3247</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>A1</td>\n",
       "      <td>92RW008</td>\n",
       "      <td>AY669703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501510</td>\n",
       "      <td>0.327678</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.427286</td>\n",
       "      <td>0.515703</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3244</td>\n",
       "      <td>1.739000</td>\n",
       "      <td>A1C</td>\n",
       "      <td>92RW009</td>\n",
       "      <td>AY669700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767172</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.345499</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.387725</td>\n",
       "      <td>0.480131</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3250</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>A1</td>\n",
       "      <td>92RW020</td>\n",
       "      <td>AY669706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639815</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.330238</td>\n",
       "      <td>0.410044</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3243</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>A1CD</td>\n",
       "      <td>92RW024</td>\n",
       "      <td>AY669699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646351</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.327785</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.340333</td>\n",
       "      <td>0.415960</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3246</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>A1C</td>\n",
       "      <td>92RW026</td>\n",
       "      <td>AY669702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652896</td>\n",
       "      <td>0.215437</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.380465</td>\n",
       "      <td>0.484519</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3245</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>92UG031</td>\n",
       "      <td>AY669701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629298</td>\n",
       "      <td>0.285732</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.296928</td>\n",
       "      <td>0.356374</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58228</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>A1</td>\n",
       "      <td>92UG037</td>\n",
       "      <td>U51190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711506</td>\n",
       "      <td>0.360390</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.385686</td>\n",
       "      <td>0.466349</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3241</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>93RW029</td>\n",
       "      <td>AY669697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660517</td>\n",
       "      <td>0.281735</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.361026</td>\n",
       "      <td>0.349914</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3248</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>A1</td>\n",
       "      <td>93UG077</td>\n",
       "      <td>AY669704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657374</td>\n",
       "      <td>0.299454</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.353569</td>\n",
       "      <td>0.478969</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.561906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3249</td>\n",
       "      <td>1.507191</td>\n",
       "      <td>A1</td>\n",
       "      <td>94UG103</td>\n",
       "      <td>AY669705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613440</td>\n",
       "      <td>0.149451</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.318889</td>\n",
       "      <td>0.414566</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3242</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>VLGC_A1</td>\n",
       "      <td>AY669698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774524</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.313681</td>\n",
       "      <td>0.345103</td>\n",
       "      <td>0.323096</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3307</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>A1C</td>\n",
       "      <td>94KE105</td>\n",
       "      <td>AY669768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.286474</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.362533</td>\n",
       "      <td>0.356242</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3313</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>01_AE</td>\n",
       "      <td>92TH021</td>\n",
       "      <td>AY669775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808720</td>\n",
       "      <td>0.377109</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.488908</td>\n",
       "      <td>0.499039</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.459603</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>58330</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>01_AE</td>\n",
       "      <td>CMU02</td>\n",
       "      <td>AY669779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770913</td>\n",
       "      <td>0.424873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.465973</td>\n",
       "      <td>0.537881</td>\n",
       "      <td>0.742702</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3260</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>B</td>\n",
       "      <td>92BR020</td>\n",
       "      <td>AY669718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583506</td>\n",
       "      <td>0.372335</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.395698</td>\n",
       "      <td>0.410970</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3270</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>B</td>\n",
       "      <td>93TH305</td>\n",
       "      <td>AY669729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397522</td>\n",
       "      <td>0.280856</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.387702</td>\n",
       "      <td>0.415098</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.417799</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5027</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>B</td>\n",
       "      <td>APV_13</td>\n",
       "      <td>DQ869019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597374</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.306181</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.351486</td>\n",
       "      <td>0.295318</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5031</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>B</td>\n",
       "      <td>APV_17</td>\n",
       "      <td>DQ869023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543128</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.306117</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.227619</td>\n",
       "      <td>0.323091</td>\n",
       "      <td>0.509443</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.348773</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5038</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>B</td>\n",
       "      <td>APV_6</td>\n",
       "      <td>DQ869030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454681</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.223867</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.451906</td>\n",
       "      <td>0.237076</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.406922</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3269</td>\n",
       "      <td>0.022549</td>\n",
       "      <td>B</td>\n",
       "      <td>JRFLclone</td>\n",
       "      <td>AY669728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575469</td>\n",
       "      <td>0.420811</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.429173</td>\n",
       "      <td>0.446369</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.329766</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6034</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>B</td>\n",
       "      <td>SF162</td>\n",
       "      <td>EU123924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442843</td>\n",
       "      <td>0.363794</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.389986</td>\n",
       "      <td>0.466077</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3261</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>B</td>\n",
       "      <td>VLGC_B3</td>\n",
       "      <td>AY669720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530226</td>\n",
       "      <td>0.342565</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.367124</td>\n",
       "      <td>0.422412</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.355278</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3276</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>NL43clone</td>\n",
       "      <td>AY669735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440401</td>\n",
       "      <td>0.423038</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.333606</td>\n",
       "      <td>0.438431</td>\n",
       "      <td>0.525802</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.373251</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2990</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>B</td>\n",
       "      <td>JRCSF</td>\n",
       "      <td>AY426127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610183</td>\n",
       "      <td>0.422708</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.355045</td>\n",
       "      <td>0.528895</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.342739</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3282</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>C</td>\n",
       "      <td>93IN905</td>\n",
       "      <td>AY669742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701792</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.312785</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.452301</td>\n",
       "      <td>0.500930</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.140174</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>58322</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>C</td>\n",
       "      <td>93MW959</td>\n",
       "      <td>AY669739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678929</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.226524</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.434045</td>\n",
       "      <td>0.500475</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3281</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>C</td>\n",
       "      <td>97ZA012</td>\n",
       "      <td>AY669741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611395</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.279774</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.516133</td>\n",
       "      <td>0.520332</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3288</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>C</td>\n",
       "      <td>98IN022</td>\n",
       "      <td>AY669748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682511</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.271638</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.350078</td>\n",
       "      <td>0.424046</td>\n",
       "      <td>0.613319</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3287</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>07_BC</td>\n",
       "      <td>98CN009</td>\n",
       "      <td>AY669747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702066</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.295613</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.333005</td>\n",
       "      <td>0.541993</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3285</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>08_BC</td>\n",
       "      <td>98CN006</td>\n",
       "      <td>AY669745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696630</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.268221</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.348193</td>\n",
       "      <td>0.429640</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>1888</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>A1</td>\n",
       "      <td>Q842-d14</td>\n",
       "      <td>AF407161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641351</td>\n",
       "      <td>0.210403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.410724</td>\n",
       "      <td>0.387143</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>1889</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>A1</td>\n",
       "      <td>Q842-d16</td>\n",
       "      <td>AF407162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.170422</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.389066</td>\n",
       "      <td>0.352833</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>58355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>D</td>\n",
       "      <td>QA013_70I_H1</td>\n",
       "      <td>FJ866134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546177</td>\n",
       "      <td>0.358337</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.313072</td>\n",
       "      <td>0.459324</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>D</td>\n",
       "      <td>QA013_70I_ENV_M12</td>\n",
       "      <td>FJ866135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546177</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.358337</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.313072</td>\n",
       "      <td>0.459324</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>14740</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>D</td>\n",
       "      <td>QA465_59M_ENV_A1</td>\n",
       "      <td>FJ866136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497040</td>\n",
       "      <td>0.287198</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.504724</td>\n",
       "      <td>0.475751</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>14741</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>D</td>\n",
       "      <td>QA465_59M_ENV_D1</td>\n",
       "      <td>FJ866137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525274</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.277310</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.473747</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>14729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1D</td>\n",
       "      <td>QA790_204I_ENV_A4</td>\n",
       "      <td>FJ866124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518774</td>\n",
       "      <td>0.302403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.529490</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>14730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1D</td>\n",
       "      <td>QA790_204I_ENV_C1</td>\n",
       "      <td>FJ866125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518774</td>\n",
       "      <td>0.302403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.529490</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>14731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1D</td>\n",
       "      <td>QA790_204I_ENV_C8</td>\n",
       "      <td>FJ866126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518774</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.302403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.529490</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>14732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1D</td>\n",
       "      <td>QA790_204I_ENV_E2</td>\n",
       "      <td>FJ866127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518774</td>\n",
       "      <td>0.302403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.259867</td>\n",
       "      <td>0.529490</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>14736</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>C</td>\n",
       "      <td>QB099_391M_ENV_B1</td>\n",
       "      <td>FJ866131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598250</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.368012</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.578953</td>\n",
       "      <td>0.424212</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>14737</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>C</td>\n",
       "      <td>QB099_391M_ENV_C8</td>\n",
       "      <td>FJ866132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598250</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.368012</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.578953</td>\n",
       "      <td>0.424212</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>14716</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QB726_70M_ENV_B3</td>\n",
       "      <td>FJ866111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685199</td>\n",
       "      <td>0.275008</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.423947</td>\n",
       "      <td>0.415590</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>14742</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>D</td>\n",
       "      <td>QB857_110I_ENV_B3</td>\n",
       "      <td>FJ866138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544168</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.368022</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.479735</td>\n",
       "      <td>0.480260</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.392859</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>14738</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>C</td>\n",
       "      <td>QC406_70M_ENV_F3</td>\n",
       "      <td>FJ866133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607566</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.345688</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.461285</td>\n",
       "      <td>0.385490</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.542557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>14743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>D</td>\n",
       "      <td>QD435_100M_ENV_A4</td>\n",
       "      <td>FJ866139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539570</td>\n",
       "      <td>0.250839</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.352421</td>\n",
       "      <td>0.403861</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.429565</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>14744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>D</td>\n",
       "      <td>QD435_100M_ENV_B5</td>\n",
       "      <td>FJ866140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539570</td>\n",
       "      <td>0.250839</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.352421</td>\n",
       "      <td>0.403861</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.408225</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>14745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>D</td>\n",
       "      <td>QD435_100M_ENV_E1</td>\n",
       "      <td>FJ866141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539570</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.250839</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.352421</td>\n",
       "      <td>0.403861</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.429565</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>14718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QF495_23M_ENV_A1</td>\n",
       "      <td>FJ866113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397599</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.497172</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500951</td>\n",
       "      <td>0.426559</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QF495_23M_ENV_A3</td>\n",
       "      <td>FJ866114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397599</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.497172</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500951</td>\n",
       "      <td>0.426559</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>14720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QF495_23M_ENV_B2</td>\n",
       "      <td>FJ866115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397599</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.497172</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500951</td>\n",
       "      <td>0.426559</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>14721</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QF495_23M_ENV_D1</td>\n",
       "      <td>FJ866116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397599</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.497172</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500951</td>\n",
       "      <td>0.426559</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>14733</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A2D</td>\n",
       "      <td>QG393_60M_ENV_A1</td>\n",
       "      <td>FJ866128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591154</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.240762</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.287014</td>\n",
       "      <td>0.469156</td>\n",
       "      <td>0.483754</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>14734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A2D</td>\n",
       "      <td>QG393_60M_ENV_B7</td>\n",
       "      <td>FJ866129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563720</td>\n",
       "      <td>0.240762</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.287014</td>\n",
       "      <td>0.469156</td>\n",
       "      <td>0.483754</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>14735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A2D</td>\n",
       "      <td>QG393_60M_ENV_B8</td>\n",
       "      <td>FJ866130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563720</td>\n",
       "      <td>0.240762</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.287014</td>\n",
       "      <td>0.469156</td>\n",
       "      <td>0.483754</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>14722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QG984_21M_ENV_A3</td>\n",
       "      <td>FJ866117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610223</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.321368</td>\n",
       "      <td>0.301785</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>14724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QH343_21M_ENV_A10</td>\n",
       "      <td>FJ866119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521856</td>\n",
       "      <td>0.155653</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.273672</td>\n",
       "      <td>0.375659</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>14725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QH343_21M_ENV_B5</td>\n",
       "      <td>FJ866120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532628</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.232385</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.273672</td>\n",
       "      <td>0.374792</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>14726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QH359_21M_ENV_C1</td>\n",
       "      <td>FJ866121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475980</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.182704</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.337606</td>\n",
       "      <td>0.391541</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>14727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>QH359_21M_ENV_D1</td>\n",
       "      <td>FJ866122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470180</td>\n",
       "      <td>0.126387</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.317805</td>\n",
       "      <td>0.391541</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows  18262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  IC50_mean  Clade               Name Accession  Pos1_-  \\\n",
       "0          3247   0.003000     A1            92RW008  AY669703     0.0   \n",
       "1          3244   1.739000    A1C            92RW009  AY669700     0.0   \n",
       "2          3250   0.004000     A1            92RW020  AY669706     0.0   \n",
       "3          3243  50.000000   A1CD            92RW024  AY669699     0.0   \n",
       "4          3246   0.014000    A1C            92RW026  AY669702     0.0   \n",
       "5          3245  50.000000     A1            92UG031  AY669701     0.0   \n",
       "6         58228   0.031000     A1            92UG037    U51190     0.0   \n",
       "7          3241  50.000000     A1            93RW029  AY669697     0.0   \n",
       "8          3248   0.019000     A1            93UG077  AY669704     0.0   \n",
       "9          3249   1.507191     A1            94UG103  AY669705     0.0   \n",
       "10         3242  50.000000     A1            VLGC_A1  AY669698     0.0   \n",
       "11         3307   0.029000    A1C            94KE105  AY669768     0.0   \n",
       "12         3313  50.000000  01_AE            92TH021  AY669775     0.0   \n",
       "13        58330  50.000000  01_AE              CMU02  AY669779     1.0   \n",
       "14         3260   0.010583      B            92BR020  AY669718     0.0   \n",
       "15         3270   0.007000      B            93TH305  AY669729     0.0   \n",
       "16         5027   0.251000      B             APV_13  DQ869019     0.0   \n",
       "17         5031   0.066000      B             APV_17  DQ869023     0.0   \n",
       "18         5038   0.018000      B              APV_6  DQ869030     0.0   \n",
       "19         3269   0.022549      B          JRFLclone  AY669728     0.0   \n",
       "20         6034   0.003281      B              SF162  EU123924     0.0   \n",
       "21         3261   0.005000      B            VLGC_B3  AY669720     0.0   \n",
       "22         3276  50.000000      B          NL43clone  AY669735     1.0   \n",
       "23         2990   0.027000      B              JRCSF  AY426127     0.0   \n",
       "24         3282   0.005000      C            93IN905  AY669742     0.0   \n",
       "25        58322   0.013000      C            93MW959  AY669739     1.0   \n",
       "26         3281   0.002000      C            97ZA012  AY669741     0.0   \n",
       "27         3288   0.030726      C            98IN022  AY669748     0.0   \n",
       "28         3287   0.009000  07_BC            98CN009  AY669747     0.0   \n",
       "29         3285   0.010000  08_BC            98CN006  AY669745     0.0   \n",
       "..          ...        ...    ...                ...       ...     ...   \n",
       "476        1888   0.004000     A1           Q842-d14  AF407161     0.0   \n",
       "477        1889   0.060000     A1           Q842-d16  AF407162     0.0   \n",
       "478       58355   1.000000      D       QA013_70I_H1  FJ866134     0.0   \n",
       "479       14739   1.000000      D  QA013_70I_ENV_M12  FJ866135     0.0   \n",
       "480       14740   0.030000      D   QA465_59M_ENV_A1  FJ866136     0.0   \n",
       "481       14741   0.030000      D   QA465_59M_ENV_D1  FJ866137     0.0   \n",
       "482       14729   1.000000    A1D  QA790_204I_ENV_A4  FJ866124     0.0   \n",
       "483       14730   1.000000    A1D  QA790_204I_ENV_C1  FJ866125     0.0   \n",
       "484       14731   1.000000    A1D  QA790_204I_ENV_C8  FJ866126     0.0   \n",
       "485       14732   1.000000    A1D  QA790_204I_ENV_E2  FJ866127     0.0   \n",
       "486       14736   0.030000      C  QB099_391M_ENV_B1  FJ866131     0.0   \n",
       "487       14737   0.030000      C  QB099_391M_ENV_C8  FJ866132     0.0   \n",
       "488       14716   0.770000     A1   QB726_70M_ENV_B3  FJ866111     0.0   \n",
       "489       14742   1.000000      D  QB857_110I_ENV_B3  FJ866138     0.0   \n",
       "490       14738   0.010000      C   QC406_70M_ENV_F3  FJ866133     0.0   \n",
       "491       14743   1.000000      D  QD435_100M_ENV_A4  FJ866139     0.0   \n",
       "492       14744   1.000000      D  QD435_100M_ENV_B5  FJ866140     0.0   \n",
       "493       14745   1.000000      D  QD435_100M_ENV_E1  FJ866141     0.0   \n",
       "494       14718   1.000000     A1   QF495_23M_ENV_A1  FJ866113     0.0   \n",
       "495       14719   1.000000     A1   QF495_23M_ENV_A3  FJ866114     0.0   \n",
       "496       14720   1.000000     A1   QF495_23M_ENV_B2  FJ866115     0.0   \n",
       "497       14721   1.000000     A1   QF495_23M_ENV_D1  FJ866116     0.0   \n",
       "498       14733   1.000000    A2D   QG393_60M_ENV_A1  FJ866128     0.0   \n",
       "499       14734   1.000000    A2D   QG393_60M_ENV_B7  FJ866129     0.0   \n",
       "500       14735   1.000000    A2D   QG393_60M_ENV_B8  FJ866130     0.0   \n",
       "501       14722   1.000000     A1   QG984_21M_ENV_A3  FJ866117     0.0   \n",
       "502       14724   1.000000     A1  QH343_21M_ENV_A10  FJ866119     0.0   \n",
       "503       14725   1.000000     A1   QH343_21M_ENV_B5  FJ866120     0.0   \n",
       "504       14726   1.000000     A1   QH359_21M_ENV_C1  FJ866121     0.0   \n",
       "505       14727   1.000000     A1   QH359_21M_ENV_D1  FJ866122     0.0   \n",
       "\n",
       "     Pos1_W  Pos1_F  Pos1_G  Pos1_A    ...         N611    N615_1      N616  \\\n",
       "0       0.0     0.0     0.0     0.0    ...     0.501510  0.327678 -1.000000   \n",
       "1       0.0     0.0     0.0     0.0    ...     0.767172 -1.000000  0.345499   \n",
       "2       0.0     0.0     0.0     0.0    ...     0.639815 -1.000000  0.223446   \n",
       "3       0.0     0.0     0.0     0.0    ...     0.646351 -1.000000  0.327785   \n",
       "4       0.0     0.0     0.0     0.0    ...     0.652896  0.215437 -1.000000   \n",
       "5       0.0     0.0     0.0     0.0    ...     0.629298  0.285732 -1.000000   \n",
       "6       0.0     0.0     0.0     0.0    ...     0.711506  0.360390 -1.000000   \n",
       "7       0.0     0.0     0.0     0.0    ...     0.660517  0.281735 -1.000000   \n",
       "8       0.0     0.0     0.0     0.0    ...     0.657374  0.299454 -1.000000   \n",
       "9       0.0     0.0     0.0     0.0    ...     0.613440  0.149451 -1.000000   \n",
       "10      0.0     0.0     0.0     0.0    ...     0.774524 -1.000000 -1.000000   \n",
       "11      0.0     0.0     0.0     0.0    ...     0.567511 -1.000000  0.286474   \n",
       "12      0.0     0.0     0.0     0.0    ...     0.808720  0.377109 -1.000000   \n",
       "13      0.0     0.0     0.0     0.0    ...     0.770913  0.424873 -1.000000   \n",
       "14      0.0     0.0     0.0     0.0    ...     0.583506  0.372335 -1.000000   \n",
       "15      0.0     0.0     0.0     0.0    ...     0.397522  0.280856 -1.000000   \n",
       "16      0.0     0.0     0.0     0.0    ...     0.597374 -1.000000  0.306181   \n",
       "17      0.0     0.0     0.0     0.0    ...     0.543128 -1.000000  0.306117   \n",
       "18      0.0     0.0     0.0     0.0    ...     0.454681 -1.000000  0.223867   \n",
       "19      0.0     0.0     0.0     0.0    ...     0.575469  0.420811 -1.000000   \n",
       "20      0.0     0.0     0.0     0.0    ...     0.442843  0.363794 -1.000000   \n",
       "21      0.0     0.0     0.0     0.0    ...     0.530226  0.342565 -1.000000   \n",
       "22      0.0     0.0     0.0     0.0    ...     0.440401  0.423038 -1.000000   \n",
       "23      0.0     0.0     0.0     0.0    ...     0.610183  0.422708 -1.000000   \n",
       "24      0.0     0.0     0.0     0.0    ...     0.701792 -1.000000  0.312785   \n",
       "25      0.0     0.0     0.0     0.0    ...     0.678929 -1.000000  0.226524   \n",
       "26      0.0     0.0     0.0     0.0    ...     0.611395 -1.000000  0.279774   \n",
       "27      0.0     0.0     0.0     0.0    ...     0.682511 -1.000000  0.271638   \n",
       "28      0.0     0.0     0.0     0.0    ...     0.702066 -1.000000  0.295613   \n",
       "29      0.0     0.0     0.0     0.0    ...     0.696630 -1.000000  0.268221   \n",
       "..      ...     ...     ...     ...    ...          ...       ...       ...   \n",
       "476     0.0     0.0     0.0     0.0    ...     0.641351  0.210403 -1.000000   \n",
       "477     0.0     0.0     0.0     0.0    ...     0.641985 -1.000000  0.170422   \n",
       "478     0.0     0.0     0.0     0.0    ...     0.546177  0.358337 -1.000000   \n",
       "479     0.0     0.0     0.0     0.0    ...     0.546177 -1.000000  0.358337   \n",
       "480     0.0     0.0     0.0     0.0    ...     0.497040  0.287198 -1.000000   \n",
       "481     0.0     0.0     0.0     0.0    ...     0.525274 -1.000000  0.277310   \n",
       "482     0.0     0.0     0.0     0.0    ...     0.518774  0.302403 -1.000000   \n",
       "483     0.0     0.0     0.0     0.0    ...     0.518774  0.302403 -1.000000   \n",
       "484     0.0     0.0     0.0     0.0    ...     0.518774 -1.000000  0.302403   \n",
       "485     0.0     0.0     0.0     0.0    ...     0.518774  0.302403 -1.000000   \n",
       "486     0.0     0.0     0.0     0.0    ...     0.598250 -1.000000  0.368012   \n",
       "487     0.0     0.0     0.0     0.0    ...     0.598250 -1.000000  0.368012   \n",
       "488     0.0     0.0     0.0     0.0    ...     0.685199  0.275008 -1.000000   \n",
       "489     0.0     0.0     0.0     0.0    ...     0.544168 -1.000000  0.368022   \n",
       "490     0.0     0.0     0.0     0.0    ...     0.607566 -1.000000  0.345688   \n",
       "491     0.0     0.0     0.0     0.0    ...     0.539570  0.250839 -1.000000   \n",
       "492     0.0     0.0     0.0     0.0    ...     0.539570  0.250839 -1.000000   \n",
       "493     0.0     0.0     0.0     0.0    ...     0.539570 -1.000000  0.250839   \n",
       "494     0.0     0.0     0.0     0.0    ...     0.397599 -1.000000  0.497172   \n",
       "495     0.0     0.0     0.0     0.0    ...     0.397599 -1.000000  0.497172   \n",
       "496     0.0     0.0     0.0     0.0    ...     0.397599 -1.000000  0.497172   \n",
       "497     0.0     0.0     0.0     0.0    ...     0.397599 -1.000000  0.497172   \n",
       "498     0.0     0.0     0.0     0.0    ...     0.591154 -1.000000  0.240762   \n",
       "499     0.0     0.0     0.0     0.0    ...     0.563720  0.240762 -1.000000   \n",
       "500     0.0     0.0     0.0     0.0    ...     0.563720  0.240762 -1.000000   \n",
       "501     0.0     0.0     0.0     0.0    ...     0.610223 -1.000000 -1.000000   \n",
       "502     0.0     0.0     0.0     0.0    ...     0.521856  0.155653 -1.000000   \n",
       "503     0.0     0.0     0.0     0.0    ...     0.532628 -1.000000  0.232385   \n",
       "504     0.0     0.0     0.0     0.0    ...     0.475980 -1.000000  0.182704   \n",
       "505     0.0     0.0     0.0     0.0    ...     0.470180  0.126387 -1.000000   \n",
       "\n",
       "         N618      N625      N637      N674      N743      N816      N824  \n",
       "0   -1.000000  0.427286  0.515703 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "1   -1.000000  0.387725  0.480131 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "2   -1.000000  0.330238  0.410044 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "3   -1.000000  0.340333  0.415960 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "4   -1.000000  0.380465  0.484519 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "5   -1.000000  0.296928  0.356374 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "6   -1.000000  0.385686  0.466349 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "7   -1.000000  0.361026  0.349914 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "8   -1.000000  0.353569  0.478969 -1.000000 -1.000000 -1.000000  0.561906  \n",
       "9   -1.000000  0.318889  0.414566 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "10   0.313681  0.345103  0.323096 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "11  -1.000000  0.362533  0.356242 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "12  -1.000000  0.488908  0.499039 -1.000000 -1.000000  0.459603 -1.000000  \n",
       "13  -1.000000  0.465973  0.537881  0.742702 -1.000000 -1.000000 -1.000000  \n",
       "14  -1.000000  0.395698  0.410970 -1.000000 -1.000000  0.370059 -1.000000  \n",
       "15  -1.000000  0.387702  0.415098 -1.000000 -1.000000  0.417799 -1.000000  \n",
       "16  -1.000000  0.351486  0.295318 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "17  -1.000000  0.227619  0.323091  0.509443 -1.000000  0.348773 -1.000000  \n",
       "18  -1.000000  0.451906  0.237076 -1.000000 -1.000000  0.406922 -1.000000  \n",
       "19  -1.000000  0.429173  0.446369 -1.000000 -1.000000  0.329766 -1.000000  \n",
       "20  -1.000000  0.389986  0.466077 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "21  -1.000000  0.367124  0.422412 -1.000000 -1.000000  0.355278 -1.000000  \n",
       "22  -1.000000  0.333606  0.438431  0.525802 -1.000000  0.373251 -1.000000  \n",
       "23  -1.000000  0.355045  0.528895 -1.000000 -1.000000  0.342739 -1.000000  \n",
       "24  -1.000000  0.452301  0.500930 -1.000000  0.140174 -1.000000 -1.000000  \n",
       "25  -1.000000  0.434045  0.500475 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "26  -1.000000  0.516133  0.520332 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "27  -1.000000  0.350078  0.424046  0.613319 -1.000000 -1.000000 -1.000000  \n",
       "28  -1.000000  0.333005  0.541993 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "29  -1.000000  0.348193  0.429640 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "476 -1.000000  0.410724  0.387143 -1.000000  0.033715 -1.000000 -1.000000  \n",
       "477 -1.000000  0.389066  0.352833 -1.000000  0.033715 -1.000000 -1.000000  \n",
       "478 -1.000000  0.313072  0.459324 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "479 -1.000000  0.313072  0.459324 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "480 -1.000000  0.504724  0.475751 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "481 -1.000000  0.473747  0.522600 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "482 -1.000000  0.259867  0.529490 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "483 -1.000000  0.259867  0.529490 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "484 -1.000000  0.259867  0.529490 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "485 -1.000000  0.259867  0.529490 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "486 -1.000000  0.578953  0.424212 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "487 -1.000000  0.578953  0.424212 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "488 -1.000000  0.423947  0.415590 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "489 -1.000000  0.479735  0.480260 -1.000000 -1.000000  0.392859 -1.000000  \n",
       "490 -1.000000  0.461285  0.385490 -1.000000 -1.000000 -1.000000  0.542557  \n",
       "491 -1.000000  0.352421  0.403861 -1.000000 -1.000000  0.429565 -1.000000  \n",
       "492 -1.000000  0.352421  0.403861 -1.000000 -1.000000  0.408225 -1.000000  \n",
       "493 -1.000000  0.352421  0.403861 -1.000000 -1.000000  0.429565 -1.000000  \n",
       "494 -1.000000  0.500951  0.426559 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "495 -1.000000  0.500951  0.426559 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "496 -1.000000  0.500951  0.426559 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "497 -1.000000  0.500951  0.426559 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "498 -1.000000  0.287014  0.469156  0.483754 -1.000000 -1.000000 -1.000000  \n",
       "499 -1.000000  0.287014  0.469156  0.483754 -1.000000 -1.000000 -1.000000  \n",
       "500 -1.000000  0.287014  0.469156  0.483754 -1.000000 -1.000000 -1.000000  \n",
       "501 -1.000000  0.321368  0.301785 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "502 -1.000000  0.273672  0.375659 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "503 -1.000000  0.273672  0.374792 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "504 -1.000000  0.337606  0.391541 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "505 -1.000000  0.317805  0.391541 -1.000000 -1.000000 -1.000000 -1.000000  \n",
       "\n",
       "[506 rows x 18262 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data.values[:, 1]\n",
    "features = data.values[:, 5:]\n",
    "labels = data.columns.values[5:]\n",
    "filter_mask = np.where(np.all(features == features[0,:], axis = 0))\n",
    "X = np.delete(features, filter_mask, axis = 1)\n",
    "X.shape\n",
    "labels = np.delete(labels, filter_mask)\n",
    "names = data.values[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning parameters for r2\n",
      "\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=3)]: Done  40 out of  40 | elapsed:  3.0min finished\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.825e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=5.124e-02, with an active set of 128 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=5.124e-02, with an active set of 128 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 138 iterations, i.e. alpha=5.078e-02, with an active set of 130 regressors, and the smallest cholesky pivot element being 7.955e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=LassoLars(alpha=1.0, copy_X=True, eps=2.2204460492503131e-16,\n",
      "     fit_intercept=True, fit_path=True, max_iter=500, normalize=True,\n",
      "     positive=False, precompute='auto', verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=3,\n",
      "       param_grid=[{'alpha': [0.01, 0.05, 0.1, 1.0]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring='r2', verbose=3)\n",
      "{'split0_test_score': array([ 0.27459551,  0.27459551,  0.22422893, -0.0026512 ]), 'split1_test_score': array([ 0.23239401,  0.31403733,  0.27595256, -0.06159674]), 'split2_test_score': array([  1.03086223e-01,   3.05005612e-01,   3.78308122e-01,\n",
      "        -2.01626455e-04]), 'split3_test_score': array([ 0.46171009,  0.46753173,  0.36875468, -0.0149722 ]), 'split4_test_score': array([ 0.17719021,  0.29334091,  0.28756365, -0.00441492]), 'split5_test_score': array([ 0.0233504 ,  0.23643156,  0.2566374 , -0.02292186]), 'split6_test_score': array([ 0.36055519,  0.28496979,  0.21715386, -0.00262072]), 'split7_test_score': array([ 0.02817842,  0.13264493,  0.14580991, -0.07453034]), 'split8_test_score': array([ 0.37661545,  0.39001342,  0.33994342, -0.0608301 ]), 'split9_test_score': array([ 0.24288857,  0.34037972,  0.27627806, -0.00831452]), 'mean_test_score': array([ 0.22829529,  0.30419184,  0.27739161, -0.0252116 ]), 'std_test_score': array([ 0.14006112,  0.08406387,  0.06834338,  0.02735189]), 'rank_test_score': array([3, 1, 2, 4]), 'split0_train_score': array([ 0.64706686,  0.64706686,  0.44672765,  0.        ]), 'split1_train_score': array([ 0.9405371 ,  0.75348703,  0.44278795,  0.        ]), 'split2_train_score': array([ 0.9255449 ,  0.76739956,  0.45387798,  0.        ]), 'split3_train_score': array([ 0.91943573,  0.75285222,  0.44008197,  0.        ]), 'split4_train_score': array([ 0.9532436 ,  0.76506149,  0.45638035,  0.        ]), 'split5_train_score': array([ 0.94036858,  0.75958732,  0.44026995,  0.        ]), 'split6_train_score': array([ 0.94235155,  0.75701398,  0.46354861,  0.        ]), 'split7_train_score': array([ 0.94099879,  0.76566484,  0.47050365,  0.        ]), 'split8_train_score': array([ 0.86415005,  0.75366602,  0.45243372,  0.        ]), 'split9_train_score': array([ 0.95442932,  0.76304974,  0.43745986,  0.        ]), 'mean_train_score': array([ 0.90281265,  0.74848491,  0.45040717,  0.        ]), 'std_train_score': array([ 0.08874483,  0.03419415,  0.01037784,  0.        ]), 'mean_fit_time': array([ 27.83478315,  12.76340313,   4.20027409,   0.30857182]), 'std_fit_time': array([ 6.18479066,  1.38811532,  0.36722228,  0.03088469]), 'mean_score_time': array([ 0.0187638 ,  0.02531877,  0.02201474,  0.01726322]), 'std_score_time': array([ 0.00544027,  0.01095068,  0.00558498,  0.00655951]), 'param_alpha': masked_array(data = [0.01 0.05 0.1 1.0],\n",
      "             mask = [False False False False],\n",
      "       fill_value = ?)\n",
      ", 'params': ({'alpha': 0.01}, {'alpha': 0.05}, {'alpha': 0.1}, {'alpha': 1.0})}\n",
      "0.2283 (+/-0.2801) for {'alpha': 0.01}\n",
      "0.3042 (+/-0.1681) for {'alpha': 0.05}\n",
      "0.2774 (+/-0.1367) for {'alpha': 0.1}\n",
      "-0.0252 (+/-0.0547) for {'alpha': 1.0}\n",
      "r2: 0.3548227864305218\n",
      "MSE: 250.8788689631305\n",
      "3.20 minutes elapsed\n",
      "3.20 minutes elapsed\n"
     ]
    }
   ],
   "source": [
    "scores = ['r2']\n",
    "# print(\"Running test {} of 3\".format(i+1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.9)\n",
    "# X_train, X_test, y_train, y_test = (X, X, Y, Y)\n",
    "t1 = time.time()\n",
    "for score in scores:\n",
    "    t3 = time.time()\n",
    "    params = [\n",
    "        {'alpha':[0.01, 0.05, 0.1, 1.0]},\n",
    "    ]\n",
    "    print(\"Tuning parameters for {}\\n\".format(score), flush = True)\n",
    "    reg = GridSearchCV(LassoLars(), params, cv=10, scoring = score, n_jobs = 3, verbose = 3)\n",
    "    reg.fit(X_train, y_train)\n",
    "    print(reg)\n",
    "#     best_params.append(\"Best parameters: {}\".format(reg.best_params_))\n",
    "#     param_counts['C'][reg.best_params_[\"C\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     print(\"Best parameters:\\n\\n{}\".format(reg.best_params_), flush = True)\n",
    "    print(reg.cv_results_, flush = True)\n",
    "    means = reg.cv_results_['mean_test_score']\n",
    "    stds = reg.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, reg.cv_results_['params']):\n",
    "        print(\"%0.4f (+/-%0.04f) for %r\" % (mean, std * 2, params), flush = True)\n",
    "\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(\"r2: {}\".format(r2_score(y_test, y_pred)))\n",
    "    print(\"MSE: {}\".format(mean_squared_error(y_test, y_pred)))\n",
    "    t4 = time.time()\n",
    "    print(\"%0.2f minutes elapsed\" %((t4-t3)/60), flush = True)\n",
    "#     r2s.append(r2_score(y_test, y_pred))\n",
    "#     mse.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"%0.2f minutes elapsed\" %((t2-t1)/60), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=5.864e-02, with an active set of 106 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=5.833e-02, with an active set of 108 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 119 iterations, i.e. alpha=5.720e-02, with an active set of 115 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=5.201e-02, with an active set of 124 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=4.225e-02, with an active set of 158 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=4.225e-02, with an active set of 158 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 181 iterations, i.e. alpha=4.049e-02, with an active set of 161 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 198 iterations, i.e. alpha=3.715e-02, with an active set of 174 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 198 iterations, i.e. alpha=3.715e-02, with an active set of 174 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 198 iterations, i.e. alpha=3.715e-02, with an active set of 174 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 241 iterations, i.e. alpha=3.102e-02, with an active set of 211 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 273 iterations, i.e. alpha=2.631e-02, with an active set of 235 regressors, and the smallest cholesky pivot element being 8.689e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 289 iterations, i.e. alpha=2.437e-02, with an active set of 247 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 293 iterations, i.e. alpha=2.412e-02, with an active set of 251 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 304 iterations, i.e. alpha=2.314e-02, with an active set of 256 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 305 iterations, i.e. alpha=2.310e-02, with an active set of 257 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 309 iterations, i.e. alpha=2.281e-02, with an active set of 261 regressors, and the smallest cholesky pivot element being 9.884e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 322 iterations, i.e. alpha=2.190e-02, with an active set of 274 regressors, and the smallest cholesky pivot element being 8.941e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 330 iterations, i.e. alpha=2.135e-02, with an active set of 276 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 330 iterations, i.e. alpha=2.135e-02, with an active set of 276 regressors, and the smallest cholesky pivot element being 9.186e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 330 iterations, i.e. alpha=2.135e-02, with an active set of 276 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 330 iterations, i.e. alpha=2.135e-02, with an active set of 276 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 332 iterations, alpha=2.135e-02, previous alpha=2.128e-02, with an active set of 277 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=6.255e-02, with an active set of 91 regressors, and the smallest cholesky pivot element being 7.376e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=5.432e-02, with an active set of 114 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 145 iterations, i.e. alpha=5.025e-02, with an active set of 139 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=4.952e-02, with an active set of 140 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 207 iterations, i.e. alpha=3.978e-02, with an active set of 187 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 213 iterations, i.e. alpha=3.940e-02, with an active set of 193 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 219 iterations, i.e. alpha=3.752e-02, with an active set of 197 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 255 iterations, alpha=3.169e-02, previous alpha=3.158e-02, with an active set of 222 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=6.346e-02, with an active set of 81 regressors, and the smallest cholesky pivot element being 8.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.344e-02, with an active set of 110 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=4.608e-02, with an active set of 128 regressors, and the smallest cholesky pivot element being 8.363e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=4.463e-02, with an active set of 135 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=4.425e-02, with an active set of 136 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 186 iterations, i.e. alpha=3.644e-02, with an active set of 166 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 187 iterations, i.e. alpha=3.643e-02, with an active set of 167 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 212 iterations, i.e. alpha=3.271e-02, with an active set of 192 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 225 iterations, i.e. alpha=3.190e-02, with an active set of 205 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 227 iterations, i.e. alpha=3.172e-02, with an active set of 207 regressors, and the smallest cholesky pivot element being 8.752e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 227 iterations, i.e. alpha=3.172e-02, with an active set of 207 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 237 iterations, i.e. alpha=3.016e-02, with an active set of 215 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 241 iterations, i.e. alpha=2.935e-02, with an active set of 219 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 244 iterations, i.e. alpha=2.902e-02, with an active set of 222 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 258 iterations, i.e. alpha=2.709e-02, with an active set of 234 regressors, and the smallest cholesky pivot element being 9.483e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 258 iterations, i.e. alpha=2.709e-02, with an active set of 234 regressors, and the smallest cholesky pivot element being 8.689e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 258 iterations, i.e. alpha=2.709e-02, with an active set of 234 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 270 iterations, i.e. alpha=2.604e-02, with an active set of 240 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 271 iterations, alpha=2.606e-02, previous alpha=2.604e-02, with an active set of 240 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=5.068e-02, with an active set of 132 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=4.725e-02, with an active set of 143 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=4.659e-02, with an active set of 145 regressors, and the smallest cholesky pivot element being 9.657e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=4.551e-02, with an active set of 147 regressors, and the smallest cholesky pivot element being 8.363e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=4.408e-02, with an active set of 150 regressors, and the smallest cholesky pivot element being 9.884e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 182 iterations, i.e. alpha=4.053e-02, with an active set of 164 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 203 iterations, i.e. alpha=3.654e-02, with an active set of 181 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 241 iterations, i.e. alpha=3.122e-02, with an active set of 207 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 254 iterations, i.e. alpha=2.940e-02, with an active set of 220 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 283 iterations, i.e. alpha=2.510e-02, with an active set of 245 regressors, and the smallest cholesky pivot element being 8.941e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 295 iterations, i.e. alpha=2.371e-02, with an active set of 251 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 296 iterations, i.e. alpha=2.358e-02, with an active set of 252 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 302 iterations, i.e. alpha=2.289e-02, with an active set of 258 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 303 iterations, i.e. alpha=2.291e-02, with an active set of 259 regressors, and the smallest cholesky pivot element being 8.689e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 315 iterations, i.e. alpha=2.207e-02, with an active set of 267 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 329 iterations, i.e. alpha=2.070e-02, with an active set of 277 regressors, and the smallest cholesky pivot element being 8.689e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 336 iterations, i.e. alpha=2.023e-02, with an active set of 274 regressors, and the smallest cholesky pivot element being 9.186e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 338 iterations, i.e. alpha=2.021e-02, with an active set of 276 regressors, and the smallest cholesky pivot element being 9.657e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 338 iterations, i.e. alpha=2.021e-02, with an active set of 276 regressors, and the smallest cholesky pivot element being 6.909e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 362 iterations, alpha=1.943e-02, previous alpha=1.942e-02, with an active set of 285 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=4.686e-02, with an active set of 138 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 191 iterations, i.e. alpha=3.658e-02, with an active set of 165 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 192 iterations, i.e. alpha=3.622e-02, with an active set of 166 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 194 iterations, i.e. alpha=3.597e-02, with an active set of 168 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 226 iterations, i.e. alpha=3.058e-02, with an active set of 196 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 232 iterations, i.e. alpha=2.988e-02, with an active set of 202 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 270 iterations, i.e. alpha=2.547e-02, with an active set of 232 regressors, and the smallest cholesky pivot element being 9.483e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 272 iterations, i.e. alpha=2.525e-02, with an active set of 234 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 274 iterations, i.e. alpha=2.509e-02, with an active set of 236 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 277 iterations, i.e. alpha=2.494e-02, with an active set of 235 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 281 iterations, alpha=2.480e-02, previous alpha=2.465e-02, with an active set of 238 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=7.011e-02, with an active set of 71 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=6.786e-02, with an active set of 75 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=4.967e-02, with an active set of 128 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=4.836e-02, with an active set of 132 regressors, and the smallest cholesky pivot element being 9.064e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=4.317e-02, with an active set of 145 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=4.190e-02, with an active set of 149 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=4.172e-02, with an active set of 150 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=3.805e-02, with an active set of 167 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 181 iterations, i.e. alpha=3.728e-02, with an active set of 171 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 212 iterations, i.e. alpha=3.246e-02, with an active set of 196 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 215 iterations, i.e. alpha=3.197e-02, with an active set of 197 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 216 iterations, i.e. alpha=3.190e-02, with an active set of 198 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 216 iterations, i.e. alpha=3.190e-02, with an active set of 198 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 216 iterations, i.e. alpha=3.190e-02, with an active set of 198 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 218 iterations, i.e. alpha=3.172e-02, with an active set of 200 regressors, and the smallest cholesky pivot element being 7.743e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 218 iterations, i.e. alpha=3.172e-02, with an active set of 200 regressors, and the smallest cholesky pivot element being 8.941e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 229 iterations, i.e. alpha=3.011e-02, with an active set of 209 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 231 iterations, i.e. alpha=3.016e-02, with an active set of 211 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 242 iterations, i.e. alpha=2.879e-02, with an active set of 218 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 251 iterations, i.e. alpha=2.792e-02, with an active set of 223 regressors, and the smallest cholesky pivot element being 9.246e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 253 iterations, i.e. alpha=2.785e-02, with an active set of 225 regressors, and the smallest cholesky pivot element being 9.186e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 257 iterations, i.e. alpha=2.759e-02, with an active set of 227 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 259 iterations, i.e. alpha=2.694e-02, with an active set of 229 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 276 iterations, i.e. alpha=2.498e-02, with an active set of 242 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 291 iterations, i.e. alpha=2.383e-02, with an active set of 255 regressors, and the smallest cholesky pivot element being 9.884e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 306 iterations, i.e. alpha=2.166e-02, with an active set of 262 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 317 iterations, i.e. alpha=2.035e-02, with an active set of 273 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 325 iterations, i.e. alpha=1.869e-02, with an active set of 279 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 344 iterations, i.e. alpha=1.690e-02, with an active set of 290 regressors, and the smallest cholesky pivot element being 9.657e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 344 iterations, i.e. alpha=1.690e-02, with an active set of 290 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 346 iterations, i.e. alpha=1.684e-02, with an active set of 292 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 347 iterations, i.e. alpha=1.681e-02, with an active set of 293 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 363 iterations, i.e. alpha=1.612e-02, with an active set of 295 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 363 iterations, i.e. alpha=1.612e-02, with an active set of 295 regressors, and the smallest cholesky pivot element being 9.599e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 369 iterations, i.e. alpha=1.595e-02, with an active set of 299 regressors, and the smallest cholesky pivot element being 8.689e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 391 iterations, i.e. alpha=1.542e-02, with an active set of 303 regressors, and the smallest cholesky pivot element being 8.093e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 401 iterations, i.e. alpha=1.518e-02, with an active set of 309 regressors, and the smallest cholesky pivot element being 8.229e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 413 iterations, i.e. alpha=1.503e-02, with an active set of 313 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 419 iterations, alpha=1.496e-02, previous alpha=1.492e-02, with an active set of 312 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=5.271e-02, with an active set of 132 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=5.168e-02, with an active set of 135 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 178 iterations, i.e. alpha=4.538e-02, with an active set of 158 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 185 iterations, i.e. alpha=4.387e-02, with an active set of 165 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 189 iterations, i.e. alpha=4.336e-02, with an active set of 167 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 196 iterations, i.e. alpha=4.224e-02, with an active set of 174 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 209 iterations, i.e. alpha=3.889e-02, with an active set of 187 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 218 iterations, i.e. alpha=3.799e-02, with an active set of 192 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 238 iterations, i.e. alpha=3.489e-02, with an active set of 208 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 264 iterations, i.e. alpha=3.056e-02, with an active set of 230 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 265 iterations, i.e. alpha=3.064e-02, with an active set of 231 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 275 iterations, i.e. alpha=2.843e-02, with an active set of 241 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 308 iterations, i.e. alpha=2.396e-02, with an active set of 268 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 330 iterations, i.e. alpha=2.224e-02, with an active set of 282 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 361 iterations, i.e. alpha=1.964e-02, with an active set of 291 regressors, and the smallest cholesky pivot element being 8.297e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 364 iterations, i.e. alpha=1.937e-02, with an active set of 294 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 366 iterations, i.e. alpha=1.919e-02, with an active set of 296 regressors, and the smallest cholesky pivot element being 8.297e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 368 iterations, i.e. alpha=1.902e-02, with an active set of 298 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 377 iterations, i.e. alpha=1.812e-02, with an active set of 305 regressors, and the smallest cholesky pivot element being 8.297e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 398 iterations, i.e. alpha=1.731e-02, with an active set of 306 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 434 iterations, alpha=1.569e-02, previous alpha=1.569e-02, with an active set of 317 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=6.125e-02, with an active set of 93 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=5.879e-02, with an active set of 99 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.678e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=4.087e-02, with an active set of 159 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 191 iterations, i.e. alpha=3.356e-02, with an active set of 181 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 194 iterations, i.e. alpha=3.326e-02, with an active set of 184 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 207 iterations, i.e. alpha=3.071e-02, with an active set of 193 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 226 iterations, i.e. alpha=2.851e-02, with an active set of 210 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 275 iterations, i.e. alpha=2.157e-02, with an active set of 251 regressors, and the smallest cholesky pivot element being 9.771e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 282 iterations, i.e. alpha=2.082e-02, with an active set of 256 regressors, and the smallest cholesky pivot element being 9.771e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 284 iterations, i.e. alpha=2.077e-02, with an active set of 258 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 286 iterations, i.e. alpha=2.059e-02, with an active set of 260 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 286 iterations, i.e. alpha=2.059e-02, with an active set of 260 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 298 iterations, i.e. alpha=1.967e-02, with an active set of 272 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 318 iterations, i.e. alpha=1.772e-02, with an active set of 286 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 327 iterations, alpha=1.700e-02, previous alpha=1.700e-02, with an active set of 296 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 110 iterations, alpha=5.988e-02, previous alpha=5.987e-02, with an active set of 103 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=5.340e-02, with an active set of 114 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=5.312e-02, with an active set of 117 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.731e-02, with an active set of 135 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=4.341e-02, with an active set of 151 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 210 iterations, i.e. alpha=3.040e-02, with an active set of 196 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 230 iterations, i.e. alpha=2.738e-02, with an active set of 214 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 231 iterations, i.e. alpha=2.723e-02, with an active set of 215 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 245 iterations, i.e. alpha=2.501e-02, with an active set of 225 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 274 iterations, i.e. alpha=2.278e-02, with an active set of 242 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 279 iterations, i.e. alpha=2.247e-02, with an active set of 247 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 279 iterations, i.e. alpha=2.242e-02, with an active set of 247 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 287 iterations, i.e. alpha=2.185e-02, with an active set of 253 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 291 iterations, i.e. alpha=2.167e-02, with an active set of 255 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 294 iterations, i.e. alpha=2.132e-02, with an active set of 258 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 295 iterations, i.e. alpha=2.091e-02, with an active set of 259 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 345 iterations, i.e. alpha=1.632e-02, with an active set of 287 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 347 iterations, i.e. alpha=1.619e-02, with an active set of 289 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 355 iterations, i.e. alpha=1.575e-02, with an active set of 293 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 355 iterations, i.e. alpha=1.575e-02, with an active set of 293 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 378 iterations, i.e. alpha=1.442e-02, with an active set of 308 regressors, and the smallest cholesky pivot element being 9.771e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 382 iterations, i.e. alpha=1.426e-02, with an active set of 310 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 388 iterations, i.e. alpha=1.394e-02, with an active set of 316 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 392 iterations, i.e. alpha=1.387e-02, with an active set of 320 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 411 iterations, i.e. alpha=1.322e-02, with an active set of 333 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 414 iterations, i.e. alpha=1.313e-02, with an active set of 336 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 424 iterations, i.e. alpha=1.263e-02, with an active set of 338 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 432 iterations, i.e. alpha=1.225e-02, with an active set of 340 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 449 iterations, i.e. alpha=1.178e-02, with an active set of 345 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 454 iterations, i.e. alpha=1.171e-02, with an active set of 348 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 481 iterations, alpha=1.103e-02, previous alpha=1.103e-02, with an active set of 352 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.985e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=6.895e-02, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lasso_lars_model = LassoLarsCV(cv = 10, verbose = 0).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.064204720267065987"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_alpha = lasso_lars_model.alpha_\n",
    "lasso_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  29   47  175  336  436  492  638  665  679  711  712  723  739  771  804\n",
      "  865  892  989 1099 1152 1241 1258 1321 1576 1625 1627 1663 1843 1887 1899\n",
      " 1907 1951 1984 1990 2015 2017 2066 2104 2115 2138 2185 2241 2459 2654 2872\n",
      " 3126 3143 3187 3260 3395 3470 3477 3682 4175 4697 5028 5116 5248 5402]\n",
      "59\n",
      "[  6.00477736   3.47589974   0.01985458   2.10352028   0.0294725\n",
      "   3.21126672   0.28417854   1.3413797    8.54007012   6.84798241\n",
      "   9.63507343   1.82091612   3.65755104   2.83596936   4.77711548\n",
      "   2.2181673    0.56098175   2.93740708   4.40103429   4.87614025\n",
      "   4.67341938   0.51618119   3.23596136   0.34421032   2.39928749\n",
      "  10.56622483   1.37363061   0.123258     7.89208617   4.02666613\n",
      "   6.39955257   6.9635553    7.85593773   0.1809886    0.40125328\n",
      "   2.9076897    7.65729974  11.95584009   5.66898737   2.96755215\n",
      "   1.48922508   0.08033039   2.63611984   2.56302771   1.23006844\n",
      "   1.08591876   2.51557264   1.89739791   6.01279849   1.61682235\n",
      "   7.48709436   1.30436326   2.90128629   2.25170405   0.58233539\n",
      "   2.26206521   2.40521695   1.03173871   5.90392238]\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "print(np.where(lasso_lars_model.coef_>0)[0])\n",
    "print(len(np.where(lasso_lars_model.coef_>0)[0]))\n",
    "print(lasso_lars_model.coef_[lasso_lars_model.coef_>0])\n",
    "print(len(lasso_lars_model.coef_[lasso_lars_model.coef_>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_lars = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(LassoLars(alpha=lasso_alpha))),\n",
    "    ('regression', LassoLars(alpha = lasso_alpha))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.019e-01, with an active set of 45 regressors, and the smallest cholesky pivot element being 7.376e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.019e-01, with an active set of 45 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.005e-01, with an active set of 48 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=9.705e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=9.354e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=9.220e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=9.151e-02, with an active set of 59 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=9.126e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=8.212e-02, with an active set of 78 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=7.885e-02, with an active set of 82 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=6.677e-02, with an active set of 108 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=6.677e-02, with an active set of 108 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=6.677e-02, with an active set of 108 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection', SelectFromModel(estimator=LassoLars(alpha=0.064204720267065987, copy_X=True, eps=2.2204460492503131e-16,\n",
       "     fit_intercept=True, fit_path=True, max_iter=500, normalize=True,\n",
       "     positive=False, precompute='auto', verbose=False),\n",
       "        prefit=False, threshold=None)), ...it_path=True, max_iter=500, normalize=True,\n",
       "     positive=False, precompute='auto', verbose=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_lars.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25676824472691651"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lasso_lars.predict(X_test)\n",
    "sklearn.metrics.r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pos6_V' 'Pos6_4_I' 'Pos6_4_H' 'Pos10_S' 'Pos20_I' 'Pos20_M' 'Pos29_N'\n",
      " 'Pos33_Q' 'Pos59_K' 'Pos78_N' 'Pos82_L' 'Pos89_I' 'Pos95_V' 'Pos132_S'\n",
      " 'Pos134_A' 'Pos134_H' 'Pos135_T' 'Pos136_G' 'Pos136_L' 'Pos136_M'\n",
      " 'Pos137_N' 'Pos139_I' 'Pos139_C' 'Pos139_D' 'Pos147_-' 'Pos147_D'\n",
      " 'Pos153_Q' 'Pos155_K' 'Pos155_E' 'Pos162_S' 'Pos171_R' 'Pos171_H'\n",
      " 'Pos173_Q' 'Pos173_H' 'Pos179_S' 'Pos183_P' 'Pos183_E' 'Pos185_-'\n",
      " 'Pos186_I' 'Pos188_A' 'Pos189_I' 'Pos194_V' 'Pos204_S' 'Pos268_-'\n",
      " 'Pos269_D' 'Pos289_A' 'Pos293_P' 'Pos297_S' 'Pos300_N' 'Pos301_Y'\n",
      " 'Pos301_E' 'Pos303_K' 'Pos306_R' 'Pos309_R' 'Pos317_Y' 'Pos321_S'\n",
      " 'Pos322_34_-' 'Pos325_K' 'Pos325_D' 'Pos330_Q' 'Pos334_S' 'Pos336_R'\n",
      " 'Pos340_T' 'Pos340_R' 'Pos342_V' 'Pos351_I' 'Pos357_N' 'Pos360_M'\n",
      " 'Pos360_N' 'Pos389_Q' 'Pos399_A' 'Pos399_M' 'Pos411_E' 'Pos412_L'\n",
      " 'Pos416_I' 'Pos417_K' 'Pos417_H' 'Pos418_X' 'Pos431_E' 'Pos460_H'\n",
      " 'Pos461_K' 'Pos461_R' 'Pos463_A' 'Pos465_-' 'Pos465_P' 'Pos465_K'\n",
      " 'Pos466_H' 'Pos524_A' 'Pos525_M' 'Pos580_I' 'Pos609_L' 'Pos612_Q'\n",
      " 'Pos640_S' 'Pos640_D' 'Pos654_D' 'Pos655_E' 'Pos665_K' 'Pos668_T'\n",
      " 'Pos721_P' 'Pos721_X' 'Pos737_G' 'Pos743_G' 'Pos744_S' 'Pos745_Y'\n",
      " 'Pos747_R' 'Pos782_L' 'Pos791_K' 'Pos794_K' 'Pos801_T' 'Pos815_L'\n",
      " 'Pos819_T' 'Pos820_I' 'Pos836_L' 'Pos836_S' 'Pos843_I' 'N186_1' 'N332']\n"
     ]
    }
   ],
   "source": [
    "inds = lasso_lars.steps[0][1].get_support(indices = True)\n",
    "\n",
    "# print(len(inds))\n",
    "# print(len(lasso_lars_model.coef_[lasso_lars_model.coef_>0]))\n",
    "print(labels[inds])\n",
    "# print(labels[np.where(lasso_lars_model.coef_!=0)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"lasso.pkl\", \"wb\") as f:\n",
    "    pkl.dump((inds, labels),f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "{'Pos818_X', 'Pos139_I', 'Pos322_34_-', 'Pos327_G', 'Pos300_A', 'Pos194_V', 'Pos336_G', 'Pos6_4_I', 'Pos134_A', 'Pos524_A', 'Pos59_K', 'Pos317_L', 'Pos6_V', 'Pos171_H', 'Pos460_H', 'Pos301_E', 'Pos415_K', 'Pos744_S', 'Pos137_C', 'Pos188_A', 'Pos313_L', 'Pos459_E', 'Pos654_D', 'Pos309_R', 'Pos325_K', 'Pos186_I', 'Pos136_L', 'Pos306_R', 'Pos525_M', 'Pos475_V', 'Pos136_M', 'Pos580_I', 'Pos317_Y'}\n"
     ]
    }
   ],
   "source": [
    "lasso_intersection = set.intersection(set(labels[inds]), set(labels[np.where(lasso_lars_model.coef_>0)[0]]))\n",
    "print(len(lasso_intersection))\n",
    "print(lasso_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos6_V\" in set.intersection(set(labels[inds]), set(labels[np.where(lasso_lars_model.coef_>0)[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elasticnet_model = ElasticNetCV(cv = 10).fit(np.ascontiguousarray(X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet_alpha = elasticnet_model.alpha_\n",
    "enet_ratio = elasticnet_model.l1_ratio_\n",
    "enet_coefs = elasticnet_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501899998257\n",
      "0.5\n",
      "[ 0.         -0.         -0.         ...,  0.         -0.         -0.05416147]\n"
     ]
    }
   ],
   "source": [
    "print(enet_alpha)\n",
    "print(enet_ratio)\n",
    "print(enet_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  14   20   44   65   97  137  143  152  168  175  206  226  232  254  292\n",
      "  314  336  353  366  373  436  529  543  548  638  665  698  699  711  743\n",
      "  771  798  804  840  885  979  989 1080 1114 1119 1182 1201 1262 1280 1305\n",
      " 1343 1414 1487 1576 1609 1624 1643 1653 1676 1756 1793 1817 1823 1843 1866\n",
      " 1886 1951 2007 2015 2017 2031 2036 2084 2102 2104 2155 2163 2169 2185 2214\n",
      " 2241 2248 2289 2321 2323 2353 2379 2457 2496 2559 2588 2622 2635 2671 2689\n",
      " 2748 2791 2807 2878 2895 2935 2972 2990 3211 3249 3281 3297 3311 3350 3366\n",
      " 3395 3682 3707 3725 3957 4012 4027 4063 4098 4139 4175 4187 4210 4241 4275\n",
      " 4336 4450 4567 4605 4613 4616 4710 4728 4745 4816 4892 4912 4917 4962 5006\n",
      " 5087 5167 5201 5247 5341 5382 5384 5393 5494 5503 5543 5544 5552 5557 5571\n",
      " 5576 5580 5585 5598 5601 5614 5620 5628]\n",
      "158\n",
      "[  3.01875293e-01   4.41808796e-02   6.75005855e-02   1.63493180e-01\n",
      "   7.81152691e-03   8.03817263e-02   7.38232226e-01   1.09193048e-02\n",
      "   1.34580695e-01   4.75839583e-01   1.90422161e-02   1.22019578e-01\n",
      "   2.06143113e-01   2.63523161e-01   3.54227762e-01   3.29199372e-02\n",
      "   3.74840101e-01   6.22406982e-03   2.76716012e-02   2.72235729e-01\n",
      "   6.81250690e-01   2.41598175e-01   3.11901248e-01   5.28387590e-02\n",
      "   5.23014323e-01   1.12020502e+00   2.08051583e-02   4.07679769e-02\n",
      "   6.57401998e-01   1.16928287e-01   2.04640679e-01   1.35682098e-01\n",
      "   7.25327968e-01   1.69839203e-02   1.03199852e+00   1.12842519e-01\n",
      "   7.26589795e-02   8.81438219e-02   3.54941450e-01   9.16517879e-01\n",
      "   6.59570675e-01   5.75609836e-01   6.40238911e-02   2.27520456e-01\n",
      "   6.30264899e-02   2.60885384e-01   1.40776517e-02   6.03790870e-02\n",
      "   1.50002811e-01   2.29456959e-01   5.85055992e-01   1.75269747e-02\n",
      "   2.61485906e-01   1.87765744e-01   2.90416377e-01   2.18817716e-01\n",
      "   5.76814885e-01   2.58826771e-01   8.36046159e-01   1.34731662e-01\n",
      "   4.33423707e-01   7.48022076e-01   7.06759220e-01   2.83272421e-01\n",
      "   2.35027022e-01   2.72639379e-01   1.83500005e-01   2.10279466e-01\n",
      "   1.13092007e-01   1.27365938e-01   7.62303137e-02   1.09501754e+00\n",
      "   2.64541134e-01   6.11309085e-01   9.20726773e-02   1.91211952e-01\n",
      "   3.11446215e-01   2.17815912e-01   3.24024559e-02   4.80675374e-01\n",
      "   2.21123257e-01   1.49617535e-01   1.44756282e-01   7.31290381e-01\n",
      "   6.66741260e-01   5.43401984e-01   3.81341227e-01   5.17125215e-01\n",
      "   3.60605106e-01   3.67960443e-01   7.14444889e-01   1.63943017e-01\n",
      "   3.01826941e-01   5.46086681e-01   6.07828067e-01   3.37921465e-01\n",
      "   3.73852413e-01   1.48858093e-01   2.75363253e-01   1.62447055e-01\n",
      "   9.08286756e-02   5.94439245e-02   2.14826316e-01   3.36195428e-01\n",
      "   4.46822099e-02   1.15500312e+00   9.14873954e-01   1.03727077e-01\n",
      "   9.88349621e-04   8.68308778e-02   6.89460046e-01   2.46792728e-01\n",
      "   7.21712961e-01   2.12671205e-01   3.07379667e-01   2.61279867e-01\n",
      "   8.27036156e-02   1.19934444e+00   7.71337367e-01   1.57334635e-01\n",
      "   3.36283540e-02   1.70143877e-02   1.51533130e-01   2.34164621e-02\n",
      "   1.79057298e-01   2.66742592e-01   3.08342842e-01   3.13710026e-01\n",
      "   2.59830019e-01   6.72474177e-02   2.48927662e-01   5.95457417e-01\n",
      "   4.33739205e-01   1.10798745e-01   1.67620821e-02   3.24968854e-01\n",
      "   3.72136889e-01   6.46831805e-01   2.46727470e-01   1.37539405e-01\n",
      "   5.14007521e-02   3.56376167e-01   1.82325651e-02   1.06065419e-01\n",
      "   9.94449886e-02   2.10958032e-01   7.31817860e-02   1.24558077e-01\n",
      "   6.78849339e-01   2.33295142e-01   2.69206775e-01   5.97832030e-01\n",
      "   7.05277498e-01   4.94837444e-01   9.75143380e-01   5.76268951e-01\n",
      "   1.10231932e-01   7.64389925e-01]\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "sigcoef_inds = np.where(enet_coefs>0)[0]\n",
    "sigcoefs = enet_coefs[enet_coefs>0]\n",
    "print(sigcoef_inds)\n",
    "print(len(sigcoef_inds))\n",
    "print(sigcoefs)\n",
    "print(len(sigcoefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(ElasticNet(alpha = enet_alpha, l1_ratio = enet_ratio))),\n",
    "    ('regression', ElasticNet(alpha = enet_alpha, l1_ratio = enet_ratio))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection', SelectFromModel(estimator=ElasticNet(alpha=0.50189999825731213, copy_X=True, fit_intercept=True,\n",
       "      l1_ratio=0.5, max_iter=1000, normalize=False, positive=False,\n",
       "      precompute=False, random_state=None, selection='cyclic', tol=0.0001,\n",
       "      warm_start=False),\n",
       "      ...      precompute=False, random_state=None, selection='cyclic', tol=0.0001,\n",
       "      warm_start=False))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.fit(X_train, y_train)\n",
    "pred = enet.predict(X_test)\n",
    "sklearn.metrics.r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13908218632641156"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "158\n",
      "['Pos2_K' 'Pos4_M' 'Pos6_I' 'Pos10_W' 'Pos18_S' 'Pos20_I' 'Pos20_M'\n",
      " 'Pos22_F' 'Pos23_W' 'Pos26_M' 'Pos29_N' 'Pos30_A' 'Pos33_Q' 'Pos46_K'\n",
      " 'Pos49_T' 'Pos50_A' 'Pos59_K' 'Pos59_R' 'Pos62_D' 'Pos62_E' 'Pos63_K'\n",
      " 'Pos84_M' 'Pos99_D' 'Pos102_D' 'Pos102_E' 'Pos106_E' 'Pos132_S' 'Pos133_N'\n",
      " 'Pos134_A' 'Pos134_V' 'Pos135_T' 'Pos135_R' 'Pos136_G' 'Pos136_V'\n",
      " 'Pos137_N' 'Pos137_D' 'Pos139_I' 'Pos139_N' 'Pos147_-' 'Pos147_D'\n",
      " 'Pos149_I' 'Pos149_T' 'Pos149_N' 'Pos151_K' 'Pos152_G' 'Pos153_G'\n",
      " 'Pos153_Q' 'Pos153_D' 'Pos155_K' 'Pos161_M' 'Pos161_T' 'Pos162_S'\n",
      " 'Pos170_Q' 'Pos171_Q' 'Pos171_R' 'Pos172_V' 'Pos172_E' 'Pos173_Y'\n",
      " 'Pos173_H' 'Pos175_L' 'Pos179_S' 'Pos181_V' 'Pos181_I' 'Pos183_P'\n",
      " 'Pos183_Q' 'Pos184_I' 'Pos185_N' 'Pos186_G' 'Pos189_T' 'Pos189_N'\n",
      " 'Pos190_E' 'Pos209_S' 'Pos209_T' 'Pos234_S' 'Pos236_T' 'Pos240_T'\n",
      " 'Pos252_K' 'Pos267_E' 'Pos268_G' 'Pos269_D' 'Pos269_E' 'Pos274_S'\n",
      " 'Pos279_N' 'Pos286_V' 'Pos287_Q' 'Pos287_H' 'Pos289_A' 'Pos289_N'\n",
      " 'Pos289_K' 'Pos290_E' 'Pos291_S' 'Pos291_T' 'Pos293_E' 'Pos297_T'\n",
      " 'Pos300_G' 'Pos300_N' 'Pos303_T' 'Pos305_K' 'Pos307_-' 'Pos307_V'\n",
      " 'Pos307_M' 'Pos309_R' 'Pos315_Q' 'Pos318_Y' 'Pos319_A' 'Pos319_T'\n",
      " 'Pos320_T' 'Pos325_N' 'Pos325_D' 'Pos330_H' 'Pos332_N' 'Pos334_S'\n",
      " 'Pos334_N' 'Pos335_G' 'Pos335_K' 'Pos337_K' 'Pos337_E' 'Pos340_T'\n",
      " 'Pos340_N' 'Pos340_R' 'Pos340_E' 'Pos343_Q' 'Pos344_E' 'Pos346_V'\n",
      " 'Pos347_S' 'Pos347_K' 'Pos347_E' 'Pos350_Q' 'Pos350_K' 'Pos353_F'\n",
      " 'Pos358_T' 'Pos358_N' 'Pos360_N' 'Pos360_R' 'Pos363_P' 'Pos364_H'\n",
      " 'Pos365_S' 'Pos373_T' 'Pos379_G' 'Pos388_S' 'Pos388_T' 'Pos389_G'\n",
      " 'Pos389_Q' 'Pos393_S' 'Pos394_T' 'Pos395_Y' 'Pos398_S' 'Pos398_N'\n",
      " 'Pos399_T' 'Pos399_N' 'Pos410_S' 'Pos411_D' 'Pos411_E' 'Pos412_G'\n",
      " 'Pos416_I' 'Pos416_L' 'Pos419_K' 'Pos419_R' 'Pos424_V' 'Pos424_I'\n",
      " 'Pos429_R' 'Pos430_V' 'Pos434_M' 'Pos440_A' 'Pos440_S' 'Pos440_R'\n",
      " 'Pos446_S' 'Pos446_T' 'Pos452_I' 'Pos459_G' 'Pos461_K' 'Pos463_D'\n",
      " 'Pos465_P' 'Pos465_T' 'Pos465_K' 'Pos474_N' 'Pos474_D' 'Pos491_I'\n",
      " 'Pos492_K' 'Pos500_K' 'Pos500_R' 'Pos500_E' 'Pos507_E' 'Pos514_V'\n",
      " 'Pos515_L' 'Pos518_V' 'Pos557_R' 'Pos565_L' 'Pos565_M' 'Pos578_A'\n",
      " 'Pos578_T' 'Pos580_V' 'Pos580_I' 'Pos585_S' 'Pos585_R' 'Pos592_L'\n",
      " 'Pos607_N' 'Pos612_A' 'Pos612_S' 'Pos613_S' 'Pos613_T' 'Pos624_N'\n",
      " 'Pos624_E' 'Pos633_K' 'Pos633_R' 'Pos636_S' 'Pos640_S' 'Pos640_N'\n",
      " 'Pos640_D' 'Pos641_T' 'Pos644_N' 'Pos648_E' 'Pos651_T' 'Pos654_D'\n",
      " 'Pos655_K' 'Pos659_D' 'Pos665_K' 'Pos671_S' 'Pos671_N' 'Pos674_D'\n",
      " 'Pos677_K' 'Pos683_K' 'Pos683_R' 'Pos700_A' 'Pos704_V' 'Pos705_I'\n",
      " 'Pos721_L' 'Pos721_T' 'Pos725_R' 'Pos730_L' 'Pos730_P' 'Pos731_E'\n",
      " 'Pos732_G' 'Pos732_R' 'Pos736_G' 'Pos737_G' 'Pos743_G' 'Pos744_R'\n",
      " 'Pos747_R' 'Pos750_S' 'Pos754_P' 'Pos770_L' 'Pos770_R' 'Pos777_I'\n",
      " 'Pos778_A' 'Pos778_V' 'Pos781_V' 'Pos781_T' 'Pos783_E' 'Pos788_L'\n",
      " 'Pos788_R' 'Pos792_A' 'Pos794_K' 'Pos798_S' 'Pos808_K' 'Pos809_N'\n",
      " 'Pos815_F' 'Pos815_V' 'Pos815_L' 'Pos820_V' 'Pos820_I' 'Pos832_I'\n",
      " 'Pos832_L' 'Pos833_I' 'Pos836_A' 'Pos836_L' 'Pos837_F' 'Pos843_I'\n",
      " 'Pos845_R' 'Pos853_A' 'N29' 'N133' 'N137' 'N139' 'N140' 'N141' 'N142_2'\n",
      " 'N142_37' 'N147' 'N160' 'N185' 'N186_1' 'N186_2' 'N186_45' 'N188' 'N234'\n",
      " 'N276' 'N289' 'N301' 'N332' 'N334' 'N339' 'N354_1' 'N356' 'N392' 'N396'\n",
      " 'N397' 'N398' 'N399' 'N403' 'N442' 'N461' 'N462' 'N463_37' 'N463_38'\n",
      " 'N465']\n",
      "['Pos4_M' 'Pos5_G' 'Pos6_3_R' 'Pos7_C' 'Pos10_W' 'Pos18_S' 'Pos19_I'\n",
      " 'Pos20_I' 'Pos22_F' 'Pos23_W' 'Pos27_M' 'Pos29_N' 'Pos30_G' 'Pos33_K'\n",
      " 'Pos46_K' 'Pos50_A' 'Pos59_K' 'Pos61_H' 'Pos62_E' 'Pos63_K' 'Pos84_M'\n",
      " 'Pos102_D' 'Pos106_E' 'Pos108_V' 'Pos132_S' 'Pos134_A' 'Pos135_K'\n",
      " 'Pos135_R' 'Pos136_L' 'Pos137_D' 'Pos139_I' 'Pos147_N' 'Pos147_D'\n",
      " 'Pos149_T' 'Pos151_K' 'Pos161_V' 'Pos162_S' 'Pos170_Q' 'Pos172_E'\n",
      " 'Pos173_Y' 'Pos181_I' 'Pos183_P' 'Pos188_Y' 'Pos189_N' 'Pos190_E'\n",
      " 'Pos198_T' 'Pos219_T' 'Pos234_S' 'Pos255_V' 'Pos266_A' 'Pos267_E'\n",
      " 'Pos269_D' 'Pos271_I' 'Pos274_S' 'Pos283_T' 'Pos289_N' 'Pos291_S'\n",
      " 'Pos292_V' 'Pos293_E' 'Pos297_I' 'Pos300_G' 'Pos306_R' 'Pos315_Q'\n",
      " 'Pos317_L' 'Pos317_Y' 'Pos319_T' 'Pos320_-' 'Pos323_T' 'Pos325_N'\n",
      " 'Pos325_K' 'Pos333_I' 'Pos334_N' 'Pos335_G' 'Pos336_G' 'Pos337_D'\n",
      " 'Pos340_T' 'Pos340_E' 'Pos344_Q' 'Pos347_N' 'Pos347_K' 'Pos350_K'\n",
      " 'Pos352_Y' 'Pos360_I' 'Pos363_P' 'Pos373_T' 'Pos379_G' 'Pos388_T'\n",
      " 'Pos389_Q' 'Pos393_S' 'Pos394_T' 'Pos398_S' 'Pos410_N' 'Pos411_N'\n",
      " 'Pos416_L' 'Pos419_K' 'Pos429_G' 'Pos434_M' 'Pos440_A' 'Pos465_N'\n",
      " 'Pos471_A' 'Pos481_S' 'Pos485_K' 'Pos491_I' 'Pos500_K' 'Pos502_K'\n",
      " 'Pos507_E' 'Pos580_I' 'Pos585_R' 'Pos588_R' 'Pos624_N' 'Pos633_K'\n",
      " 'Pos636_S' 'Pos640_D' 'Pos644_N' 'Pos648_E' 'Pos654_D' 'Pos655_R'\n",
      " 'Pos659_D' 'Pos665_K' 'Pos671_N' 'Pos683_R' 'Pos705_V' 'Pos725_R'\n",
      " 'Pos730_P' 'Pos731_E' 'Pos732_G' 'Pos746_V' 'Pos747_R' 'Pos750_S'\n",
      " 'Pos762_S' 'Pos774_F' 'Pos777_I' 'Pos778_A' 'Pos783_E' 'Pos788_R'\n",
      " 'Pos798_S' 'Pos808_K' 'Pos812_I' 'Pos818_T' 'Pos831_E' 'Pos836_F'\n",
      " 'Pos836_A' 'Pos837_F' 'Pos851_L' 'Pos853_A' 'N140' 'N141' 'N142_37' 'N147'\n",
      " 'N187' 'N234' 'N289' 'N334' 'N392' 'N397' 'N410' 'N448' 'N465']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enet_inds = enet.steps[0][1].get_support(indices = True)\n",
    "# print(len(enet_inds))\n",
    "# print(len(sigcoefs))\n",
    "# print(labels[enet_inds])\n",
    "# print(labels[sigcoef_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"enet.pkl\", 'wb') as f:\n",
    "    pkl.dump((enet_inds, labels), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "{'Pos151_K', 'Pos419_K', 'Pos352_Y', 'Pos774_F', 'Pos102_D', 'Pos63_K', 'Pos747_R', 'Pos812_I', 'Pos132_S', 'Pos134_A', 'Pos84_M', 'Pos59_K', 'Pos778_A', 'Pos836_A', 'Pos10_W', 'Pos274_S', 'Pos173_Y', 'Pos347_K', 'Pos292_V', 'Pos136_L', 'Pos360_I', 'Pos61_H', 'Pos831_E', 'Pos135_K', 'Pos5_G', 'N147', 'Pos323_T', 'Pos429_G', 'Pos336_G', 'Pos705_V', 'Pos398_S', 'Pos732_G', 'Pos317_L', 'Pos18_S', 'Pos783_E', 'Pos172_E', 'Pos350_K', 'Pos137_D', 'Pos22_F', 'Pos319_T', 'N465', 'Pos502_K', 'Pos347_N', 'Pos389_Q', 'Pos648_E', 'Pos335_G', 'Pos170_Q', 'Pos30_G', 'Pos363_P', 'N334', 'Pos633_K', 'Pos588_R', 'Pos762_S', 'Pos683_R', 'N142_37', 'Pos108_V', 'Pos255_V', 'Pos394_T', 'Pos317_Y', 'Pos640_D', 'N140', 'Pos293_E', 'N289', 'N397', 'Pos19_I', 'Pos507_E', 'N392', 'Pos393_S', 'Pos379_G', 'Pos325_N', 'Pos315_Q', 'Pos50_A', 'Pos434_M', 'Pos777_I', 'Pos659_D', 'Pos746_V', 'Pos320_-', 'Pos266_A', 'Pos654_D', 'Pos344_Q', 'Pos491_I', 'Pos373_T', 'Pos340_T', 'Pos149_T', 'Pos325_K', 'Pos388_T', 'Pos181_I', 'Pos23_W', 'Pos416_L', 'Pos198_T', 'Pos465_N', 'Pos291_S', 'Pos219_T', 'Pos636_S', 'Pos485_K', 'Pos147_N', 'Pos665_K', 'Pos340_E', 'Pos671_N', 'Pos334_N', 'Pos297_I', 'Pos300_G', 'Pos481_S', 'N234', 'N187', 'Pos188_Y', 'Pos306_R', 'Pos580_I', 'Pos750_S'}\n"
     ]
    }
   ],
   "source": [
    "enet_intersection = set.intersection(set(labels[enet_inds]), set(labels[sigcoef_inds]))\n",
    "print(len(enet_intersection))\n",
    "print(enet_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pos59_K', 'Pos134_A', 'Pos136_L', 'Pos306_R', 'Pos317_L', 'Pos317_Y', 'Pos325_K', 'Pos336_G', 'Pos580_I', 'Pos654_D']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "overall_int = sorted(sorted(set.intersection(lasso_intersection, enet_intersection)), key = len)\n",
    "print(overall_int)\n",
    "print(len(overall_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501899998257\n"
     ]
    }
   ],
   "source": [
    "print(enet_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning parameters for r2\n",
      "\n",
      "Fitting 10 folds for each of 44 candidates, totalling 440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=4)]: Done 440 out of 440 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=4,\n",
      "       param_grid=[{'alpha': [0.3, 0.4, 0.45, 0.5], 'l1_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring='r2', verbose=3)\n",
      "{'split0_test_score': array([ 0.29247483,  0.29383907,  0.2991721 ,  0.30085835,  0.30347891,\n",
      "        0.30491274,  0.30238648,  0.29687666,  0.29825432,  0.29967897,\n",
      "        0.27542626,  0.28927248,  0.2886521 ,  0.29287918,  0.29494344,\n",
      "        0.29698352,  0.2961817 ,  0.29611375,  0.29855711,  0.30574639,\n",
      "        0.30790185,  0.27232019,  0.28715346,  0.28605336,  0.28910681,\n",
      "        0.29169306,  0.29263634,  0.29230777,  0.29596645,  0.30008144,\n",
      "        0.30466076,  0.30568288,  0.27610083,  0.28487001,  0.28332313,\n",
      "        0.2854588 ,  0.28770132,  0.28836036,  0.29029348,  0.2949236 ,\n",
      "        0.29938037,  0.30071469,  0.29935553,  0.27741376]), 'split1_test_score': array([ 0.1404945 ,  0.14450194,  0.14935753,  0.1564043 ,  0.16830108,\n",
      "        0.18083858,  0.20140507,  0.22975574,  0.25761764,  0.27819173,\n",
      "        0.27829647,  0.16748281,  0.17397226,  0.17965914,  0.19011387,\n",
      "        0.20351336,  0.21806434,  0.23751456,  0.26017606,  0.28459339,\n",
      "        0.30182972,  0.31921696,  0.17710005,  0.18388166,  0.18997744,\n",
      "        0.20230745,  0.21470596,  0.23051098,  0.24896871,  0.26828534,\n",
      "        0.29192169,  0.31313118,  0.33503801,  0.18491637,  0.19171131,\n",
      "        0.19852654,  0.21167283,  0.22294171,  0.23872307,  0.25493347,\n",
      "        0.27364715,  0.29736642,  0.31888814,  0.34689303]), 'split2_test_score': array([ 0.30790546,  0.29568545,  0.29092969,  0.28959421,  0.29067428,\n",
      "        0.2946063 ,  0.30238425,  0.31160743,  0.31747671,  0.32872849,\n",
      "        0.34564027,  0.29315147,  0.28009002,  0.27526286,  0.27306875,\n",
      "        0.27343404,  0.27745316,  0.28283374,  0.28301767,  0.28290734,\n",
      "        0.28872668,  0.29965493,  0.28678918,  0.27348259,  0.26861636,\n",
      "        0.26577727,  0.26669769,  0.26979715,  0.27133814,  0.26628279,\n",
      "        0.2659257 ,  0.2694577 ,  0.28147797,  0.28095105,  0.26754935,\n",
      "        0.26265455,  0.25919498,  0.26031716,  0.26189829,  0.25850586,\n",
      "        0.25142374,  0.25249002,  0.25820366,  0.2693255 ]), 'split3_test_score': array([ 0.30956748,  0.30589624,  0.30752327,  0.31385236,  0.31917756,\n",
      "        0.33156359,  0.34414972,  0.34867849,  0.3516815 ,  0.36803278,\n",
      "        0.37859712,  0.31345791,  0.31332269,  0.31841298,  0.32750102,\n",
      "        0.34056329,  0.35073033,  0.35842501,  0.36594788,  0.37470761,\n",
      "        0.38993748,  0.40088877,  0.3142009 ,  0.31511833,  0.32113582,\n",
      "        0.33202222,  0.34487068,  0.35462443,  0.36279684,  0.37123993,\n",
      "        0.38050882,  0.39212909,  0.40314588,  0.31440634,  0.31605089,\n",
      "        0.32290259,  0.33513691,  0.34737972,  0.35639081,  0.36536357,\n",
      "        0.3748758 ,  0.38176736,  0.39472512,  0.41230823]), 'split4_test_score': array([ 0.1724858 ,  0.15182471,  0.13723082,  0.11561907,  0.0938741 ,\n",
      "        0.07416535,  0.05869348,  0.04339385,  0.01177651, -0.04488634,\n",
      "       -0.09415732,  0.17641605,  0.15871028,  0.14253949,  0.12106903,\n",
      "        0.10354251,  0.08997846,  0.07926848,  0.06042533,  0.03805885,\n",
      "        0.00560654, -0.03469913,  0.17676815,  0.15998724,  0.14213109,\n",
      "        0.1210841 ,  0.10621823,  0.09355232,  0.0808023 ,  0.06522195,\n",
      "        0.04847219,  0.0162214 , -0.02365181,  0.17644397,  0.1600792 ,\n",
      "        0.14026538,  0.11995289,  0.10692541,  0.09362543,  0.0808748 ,\n",
      "        0.06798798,  0.05621516,  0.01988045, -0.0243177 ]), 'split5_test_score': array([ 0.17612003,  0.18256806,  0.18783173,  0.18913899,  0.1869313 ,\n",
      "        0.17975457,  0.17432116,  0.17790582,  0.18601466,  0.19012631,\n",
      "        0.17054505,  0.18220694,  0.18966471,  0.19441177,  0.19357044,\n",
      "        0.18944518,  0.18170713,  0.18247064,  0.19013711,  0.19538951,\n",
      "        0.19461377,  0.19032565,  0.18359501,  0.19213793,  0.19465263,\n",
      "        0.19397818,  0.18863881,  0.18293793,  0.18828358,  0.19376054,\n",
      "        0.19652649,  0.19849024,  0.19718071,  0.1843075 ,  0.19355324,\n",
      "        0.19492175,  0.1930895 ,  0.18800335,  0.18534119,  0.19181519,\n",
      "        0.19521327,  0.19839074,  0.20038802,  0.1979419 ]), 'split6_test_score': array([ 0.22427017,  0.21316808,  0.20396733,  0.20501574,  0.21163132,\n",
      "        0.21954119,  0.22816856,  0.23750273,  0.25472542,  0.26838264,\n",
      "        0.2852208 ,  0.22180226,  0.20825545,  0.20132122,  0.20700533,\n",
      "        0.21708803,  0.22452713,  0.23433667,  0.24977087,  0.25894284,\n",
      "        0.28006578,  0.30268792,  0.22034521,  0.20665304,  0.20090691,\n",
      "        0.20870211,  0.2194373 ,  0.22586024,  0.23884617,  0.25161904,\n",
      "        0.26281253,  0.27987224,  0.30471733,  0.21883956,  0.20508902,\n",
      "        0.20033093,  0.20960125,  0.22037674,  0.22884419,  0.24249622,\n",
      "        0.25300486,  0.26247518,  0.28038074,  0.30549832]), 'split7_test_score': array([ 0.41986624,  0.4127758 ,  0.404873  ,  0.39791035,  0.39109894,\n",
      "        0.38705886,  0.38009098,  0.37619546,  0.37331833,  0.36309582,\n",
      "        0.34269281,  0.42216417,  0.4123369 ,  0.40345743,  0.39674751,\n",
      "        0.38927956,  0.38383162,  0.37670734,  0.37214225,  0.36864502,\n",
      "        0.3601237 ,  0.34133442,  0.42166317,  0.41061957,  0.40064311,\n",
      "        0.39388139,  0.38551663,  0.37896899,  0.37145196,  0.36520068,\n",
      "        0.35922148,  0.34955565,  0.33440296,  0.42047352,  0.40832348,\n",
      "        0.3973558 ,  0.39003781,  0.38046057,  0.37213321,  0.36473395,\n",
      "        0.35532929,  0.34731947,  0.33837331,  0.33172579]), 'split8_test_score': array([ 0.21307681,  0.20141889,  0.19923946,  0.20081111,  0.20391543,\n",
      "        0.20679389,  0.20613329,  0.20465031,  0.20159456,  0.20264408,\n",
      "        0.19051424,  0.21800077,  0.20603636,  0.20171367,  0.20376058,\n",
      "        0.20332458,  0.20290878,  0.20018529,  0.19893895,  0.20423279,\n",
      "        0.21966372,  0.23603307,  0.21889227,  0.20644347,  0.20184488,\n",
      "        0.20307976,  0.20074712,  0.19999629,  0.19542532,  0.19478055,\n",
      "        0.20544036,  0.23071803,  0.25062534,  0.21909806,  0.20592498,\n",
      "        0.2009726 ,  0.20037365,  0.19775597,  0.19482015,  0.19001474,\n",
      "        0.19203829,  0.20879897,  0.23737191,  0.25513949]), 'split9_test_score': array([ 0.18502596,  0.17130234,  0.15496812,  0.15257598,  0.1546053 ,\n",
      "        0.15737433,  0.16583003,  0.17175039,  0.18224579,  0.19897484,\n",
      "        0.20523971,  0.17031501,  0.15310877,  0.14021028,  0.13943948,\n",
      "        0.13815818,  0.14467366,  0.14779577,  0.15741004,  0.17353985,\n",
      "        0.18584117,  0.19843257,  0.16352737,  0.14493248,  0.13376156,\n",
      "        0.13248934,  0.13417849,  0.13880727,  0.14229576,  0.15407108,\n",
      "        0.1672121 ,  0.17910297,  0.19297257,  0.15714048,  0.13754616,\n",
      "        0.12789913,  0.1265157 ,  0.1306412 ,  0.13298101,  0.13946218,\n",
      "        0.14961839,  0.1614995 ,  0.17280018,  0.18484083]), 'mean_test_score': array([ 0.24413375,  0.23730961,  0.23354593,  0.23221198,  0.23239885,\n",
      "        0.23370002,  0.23641617,  0.23990016,  0.2435133 ,  0.2453041 ,\n",
      "        0.2377901 ,  0.24545478,  0.23846479,  0.23506113,  0.23479466,\n",
      "        0.23562195,  0.23710976,  0.23968893,  0.24376191,  0.24877005,\n",
      "        0.25349005,  0.25260697,  0.24504083,  0.23799441,  0.23436465,\n",
      "        0.23459023,  0.23547089,  0.23686186,  0.23975331,  0.24317706,\n",
      "        0.24838041,  0.25350085,  0.25519242,  0.24419054,  0.23699011,\n",
      "        0.23322587,  0.23343103,  0.23443565,  0.23564444,  0.23845091,\n",
      "        0.2413861 ,  0.24682471,  0.25210455,  0.25568403]), 'std_test_score': array([ 0.08171214,  0.08191509,  0.08306216,  0.08447051,  0.08595114,\n",
      "        0.08940377,  0.09156211,  0.09358823,  0.10019528,  0.11518049,\n",
      "        0.12928028,  0.07848792,  0.07874128,  0.08037309,  0.08200716,\n",
      "        0.08443401,  0.08635246,  0.08780953,  0.09082006,  0.09558818,\n",
      "        0.1042042 ,  0.11405024,  0.07759339,  0.07797895,  0.07967891,\n",
      "        0.08163147,  0.08351336,  0.08535247,  0.08732122,  0.08968167,\n",
      "        0.09318685,  0.10087011,  0.11141548,  0.07697352,  0.0775414 ,\n",
      "        0.07929851,  0.0814968 ,  0.08275681,  0.08479492,  0.08674279,\n",
      "        0.08876182,  0.09048364,  0.09955896,  0.11383042]), 'rank_test_score': array([14, 26, 40, 44, 43, 39, 30, 19, 16, 11, 25, 10, 22, 34, 35, 32, 27,\n",
      "       21, 15,  7,  4,  5, 12, 24, 38, 36, 33, 29, 20, 17,  8,  3,  2, 13,\n",
      "       28, 42, 41, 37, 31, 23, 18,  9,  6,  1]), 'split0_train_score': array([ 0.82807894,  0.79131097,  0.75912115,  0.73159525,  0.70756251,\n",
      "        0.68622696,  0.66846164,  0.65459347,  0.64572168,  0.64125184,\n",
      "        0.64413159,  0.78198971,  0.73354781,  0.69296212,  0.65876317,\n",
      "        0.62868308,  0.60317693,  0.58310603,  0.56877664,  0.55943713,\n",
      "        0.55662661,  0.5614593 ,  0.76148955,  0.70784331,  0.66382195,\n",
      "        0.62672733,  0.59451793,  0.56821109,  0.54778445,  0.53277301,\n",
      "        0.5232391 ,  0.52008425,  0.52312424,  0.74243512,  0.68398604,\n",
      "        0.63690949,  0.59721777,  0.56370457,  0.53684303,  0.51624171,\n",
      "        0.50036585,  0.49009607,  0.48752756,  0.49278744]), 'split1_train_score': array([ 0.83753392,  0.80246957,  0.77215054,  0.74637495,  0.72403976,\n",
      "        0.70452592,  0.68894471,  0.67657658,  0.66664541,  0.66430801,\n",
      "        0.67118836,  0.79186032,  0.74539554,  0.70691643,  0.67443348,\n",
      "        0.64643413,  0.62370845,  0.60444616,  0.58940189,  0.57906578,\n",
      "        0.57455254,  0.57750809,  0.771429  ,  0.71980688,  0.67784707,\n",
      "        0.64246553,  0.61293048,  0.58850038,  0.56818358,  0.552994  ,\n",
      "        0.5421886 ,  0.53751186,  0.54393431,  0.75238386,  0.69599994,\n",
      "        0.65082966,  0.6130328 ,  0.58223851,  0.55636586,  0.53574921,\n",
      "        0.51942563,  0.50914982,  0.50532087,  0.51124291]), 'split2_train_score': array([ 0.82960612,  0.79340584,  0.76192932,  0.73566384,  0.71245634,\n",
      "        0.69172293,  0.67400969,  0.65960084,  0.64982397,  0.64425679,\n",
      "        0.64778967,  0.78439503,  0.73695538,  0.69749732,  0.66440058,\n",
      "        0.6350358 ,  0.60978795,  0.58896811,  0.57156852,  0.55914223,\n",
      "        0.55392319,  0.55621846,  0.76427929,  0.71177967,  0.66896445,\n",
      "        0.63298903,  0.60140408,  0.57517384,  0.55254636,  0.53405708,\n",
      "        0.52294268,  0.5171898 ,  0.51874392,  0.74557376,  0.68832431,\n",
      "        0.6424722 ,  0.60395939,  0.570749  ,  0.54331474,  0.51941004,\n",
      "        0.50168596,  0.49109936,  0.48666348,  0.48931226]), 'split3_train_score': array([ 0.82903955,  0.79297408,  0.76183959,  0.73523008,  0.71168875,\n",
      "        0.69118698,  0.67418774,  0.66222051,  0.65464085,  0.6517911 ,\n",
      "        0.65949869,  0.78247452,  0.73476146,  0.69497332,  0.66086559,\n",
      "        0.63124878,  0.60755055,  0.58941617,  0.57553715,  0.56466749,\n",
      "        0.55941093,  0.56559463,  0.76172718,  0.70876167,  0.66527341,\n",
      "        0.62797946,  0.59686348,  0.5722229 ,  0.55296822,  0.53771281,\n",
      "        0.52670025,  0.52289902,  0.52883427,  0.74242743,  0.68455537,\n",
      "        0.63763863,  0.59765515,  0.56568687,  0.5400752 ,  0.51992186,\n",
      "        0.50354234,  0.49391891,  0.49197309,  0.4992403 ]), 'split4_train_score': array([ 0.82902205,  0.79367989,  0.76287476,  0.73650112,  0.7141142 ,\n",
      "        0.69534735,  0.67994712,  0.66890343,  0.66066711,  0.65588343,\n",
      "        0.66254541,  0.78305708,  0.73626723,  0.69725969,  0.66444177,\n",
      "        0.63675644,  0.61351566,  0.59511795,  0.58068845,  0.5723191 ,\n",
      "        0.56985348,  0.57540848,  0.76258297,  0.71069318,  0.66824694,\n",
      "        0.6327679 ,  0.60294747,  0.57879046,  0.55878153,  0.54481813,\n",
      "        0.53723161,  0.53523595,  0.54173011,  0.74354043,  0.68699881,\n",
      "        0.64136236,  0.60362882,  0.57236398,  0.54712782,  0.52672839,\n",
      "        0.5135702 ,  0.50610687,  0.50411015,  0.50921598]), 'split5_train_score': array([ 0.83198425,  0.7964551 ,  0.76527393,  0.73855824,  0.71487959,\n",
      "        0.69528937,  0.67944579,  0.6668378 ,  0.65844851,  0.65443009,\n",
      "        0.65642762,  0.7865318 ,  0.73970506,  0.70031587,  0.66670896,\n",
      "        0.63864108,  0.61519931,  0.59603814,  0.58273246,  0.57321237,\n",
      "        0.56789732,  0.57386113,  0.76627201,  0.71431187,  0.67153225,\n",
      "        0.63527485,  0.60552737,  0.58072745,  0.56128522,  0.54725419,\n",
      "        0.53685664,  0.5335635 ,  0.53971607,  0.74742074,  0.69067817,\n",
      "        0.644863  ,  0.60648642,  0.57509991,  0.54989561,  0.53021849,\n",
      "        0.51523747,  0.50496596,  0.50301084,  0.50952616]), 'split6_train_score': array([ 0.82989025,  0.79312346,  0.7611348 ,  0.73400777,  0.71092942,\n",
      "        0.69212341,  0.6768078 ,  0.66488343,  0.65744624,  0.65390871,\n",
      "        0.65727206,  0.78438373,  0.73633024,  0.69641502,  0.6632946 ,\n",
      "        0.63601703,  0.61312374,  0.59469243,  0.58030336,  0.57004995,\n",
      "        0.56554139,  0.56961954,  0.76414845,  0.71105612,  0.66790771,\n",
      "        0.63248641,  0.60341062,  0.57900205,  0.55970387,  0.54455928,\n",
      "        0.53493976,  0.53225143,  0.53540812,  0.74534264,  0.68761688,\n",
      "        0.64152785,  0.60423298,  0.57339483,  0.54819914,  0.52821441,\n",
      "        0.51315321,  0.50368684,  0.50141331,  0.50219479]), 'split7_train_score': array([ 0.82657431,  0.78993263,  0.75808765,  0.73100807,  0.70737445,\n",
      "        0.68789943,  0.67287848,  0.6615294 ,  0.65401488,  0.65211965,\n",
      "        0.65998343,  0.7791387 ,  0.73071373,  0.69013645,  0.65594356,\n",
      "        0.62739252,  0.60420713,  0.58605998,  0.57300288,  0.56415937,\n",
      "        0.55954519,  0.56483263,  0.75800466,  0.70422875,  0.65992079,\n",
      "        0.62294726,  0.59281089,  0.56799397,  0.5495941 ,  0.53576449,\n",
      "        0.5260336 ,  0.52262298,  0.52970118,  0.73834832,  0.67959952,\n",
      "        0.63202595,  0.59288612,  0.56087699,  0.53538403,  0.51662688,\n",
      "        0.50218913,  0.49222305,  0.49068613,  0.49676503]), 'split8_train_score': array([ 0.83037489,  0.79490637,  0.76409444,  0.73806164,  0.71504739,\n",
      "        0.69545659,  0.67923566,  0.66791253,  0.66266398,  0.66519303,\n",
      "        0.67688797,  0.78490322,  0.7383214 ,  0.6992701 ,  0.66620395,\n",
      "        0.63794277,  0.61489113,  0.59693491,  0.58403281,  0.57817188,\n",
      "        0.57734329,  0.57915037,  0.76463891,  0.71302811,  0.67038042,\n",
      "        0.63449512,  0.6047784 ,  0.58091212,  0.56180421,  0.54920364,\n",
      "        0.5420166 ,  0.53933772,  0.541518  ,  0.74578271,  0.68945623,\n",
      "        0.64353518,  0.60551531,  0.57482042,  0.54949826,  0.53091504,\n",
      "        0.5177392 ,  0.50939934,  0.50512238,  0.50876321]), 'split9_train_score': array([ 0.8278077 ,  0.79135408,  0.75958942,  0.73308618,  0.70987221,\n",
      "        0.69010182,  0.67446809,  0.66256819,  0.65440662,  0.64989841,\n",
      "        0.65368413,  0.78271634,  0.73513709,  0.69567979,  0.66276773,\n",
      "        0.63493024,  0.61265386,  0.59473966,  0.58054722,  0.57069827,\n",
      "        0.56751053,  0.57369853,  0.76273416,  0.71018103,  0.66762153,\n",
      "        0.63212639,  0.60297895,  0.57972943,  0.56108257,  0.54662467,\n",
      "        0.53721237,  0.53520554,  0.54135596,  0.74419773,  0.68702833,\n",
      "        0.64170628,  0.60414163,  0.57407122,  0.54981806,  0.53119399,\n",
      "        0.51643577,  0.50834292,  0.50588588,  0.50919633]), 'mean_train_score': array([ 0.8299912 ,  0.7939612 ,  0.76260956,  0.73600871,  0.71279646,\n",
      "        0.69298807,  0.67683867,  0.66456262,  0.65644793,  0.6533041 ,\n",
      "        0.65894089,  0.78414505,  0.73671349,  0.69714261,  0.66378234,\n",
      "        0.63530819,  0.61178147,  0.59295196,  0.57865914,  0.56909236,\n",
      "        0.56522045,  0.56973512,  0.76373062,  0.71116906,  0.66815165,\n",
      "        0.63202593,  0.60181697,  0.57712637,  0.55737341,  0.54257613,\n",
      "        0.53293612,  0.5295902 ,  0.53440662,  0.74474527,  0.68742436,\n",
      "        0.64128706,  0.60287564,  0.57130063,  0.54565218,  0.525522  ,\n",
      "        0.51033448,  0.50089891,  0.49817137,  0.50282444]), 'std_train_score': array([ 0.00288297,  0.00334095,  0.00381278,  0.00419727,  0.0045577 ,\n",
      "        0.00485394,  0.00524662,  0.00567755,  0.00581128,  0.00716325,\n",
      "        0.00931069,  0.00318812,  0.00373673,  0.00429186,  0.00474454,\n",
      "        0.00517379,  0.00567854,  0.00584154,  0.00600784,  0.00670023,\n",
      "        0.00731758,  0.00711594,  0.00333402,  0.00393138,  0.00454076,\n",
      "        0.00504191,  0.00557053,  0.00600392,  0.00606744,  0.0066213 ,\n",
      "        0.00710365,  0.00762555,  0.00835627,  0.00347861,  0.00413186,\n",
      "        0.00478319,  0.00536112,  0.00598184,  0.00627059,  0.00656185,\n",
      "        0.00709795,  0.00764139,  0.00753867,  0.00752187]), 'mean_fit_time': array([ 12.85585537,   5.69319446,   3.11596968,   2.23646798,\n",
      "         1.45576611,   0.97815225,   0.84189777,   0.66467133,\n",
      "         0.54384778,   0.5568948 ,   0.46698246,  15.95932763,\n",
      "         3.71226542,   1.49251387,   1.04427354,   0.78275521,\n",
      "         0.74422863,   0.63831515,   0.50881326,   0.41014347,\n",
      "         0.37926905,   0.36690888,  14.05918989,   2.90969434,\n",
      "         1.17013092,   0.88908186,   0.59692316,   0.48114192,\n",
      "         0.51706643,   0.40233521,   0.32007971,   0.29732673,\n",
      "         0.34789689,  15.0443419 ,   2.50038471,   1.19840469,\n",
      "         0.69984601,   0.5180927 ,   0.4139936 ,   0.34774666,\n",
      "         0.31547375,   0.27974894,   0.27179308,   0.33849013]), 'std_fit_time': array([ 1.19542426,  0.55618531,  0.64211126,  0.82065588,  0.28598803,\n",
      "        0.13746979,  0.0840439 ,  0.09308463,  0.09992685,  0.06620271,\n",
      "        0.09763199,  1.28296806,  0.49257219,  0.21154472,  0.10712787,\n",
      "        0.08203656,  0.15521183,  0.200038  ,  0.11810858,  0.0877983 ,\n",
      "        0.08568319,  0.19004281,  1.44165279,  1.21834806,  0.10468912,\n",
      "        0.19898569,  0.04726993,  0.03925781,  0.11638901,  0.05311036,\n",
      "        0.02277047,  0.01717861,  0.17028539,  1.99075765,  0.5494989 ,\n",
      "        0.17802596,  0.05916362,  0.05623454,  0.05367581,  0.03538683,\n",
      "        0.0202468 ,  0.01897872,  0.0131929 ,  0.1725283 ]), 'mean_score_time': array([ 0.01721215,  0.01576126,  0.01421058,  0.01165879,  0.01156025,\n",
      "        0.01175838,  0.01100814,  0.01055741,  0.01045785,  0.01190944,\n",
      "        0.01115804,  0.0192636 ,  0.01516161,  0.01140828,  0.01085808,\n",
      "        0.01085784,  0.01260939,  0.01265936,  0.01385951,  0.01110823,\n",
      "        0.01290946,  0.01010709,  0.01491053,  0.0148617 ,  0.01030731,\n",
      "        0.01050742,  0.01020765,  0.01020737,  0.01260922,  0.01145842,\n",
      "        0.01085722,  0.00985758,  0.01015739,  0.01836295,  0.01546149,\n",
      "        0.01205876,  0.01145916,  0.01180837,  0.01105812,  0.01080825,\n",
      "        0.01015718,  0.00960641,  0.0125586 ,  0.01020761]), 'std_score_time': array([ 0.0054815 ,  0.00325925,  0.0041147 ,  0.00136189,  0.00307899,\n",
      "        0.00386461,  0.00134222,  0.00121378,  0.00098657,  0.00226869,\n",
      "        0.00095048,  0.00555978,  0.00239986,  0.00139387,  0.00159855,\n",
      "        0.00092383,  0.00219034,  0.00217006,  0.00430386,  0.00263596,\n",
      "        0.00367548,  0.00088947,  0.00258764,  0.00414409,  0.00045849,\n",
      "        0.00107324,  0.00124987,  0.00089852,  0.00405138,  0.00093498,\n",
      "        0.00219151,  0.00055075,  0.00039063,  0.00342291,  0.00410435,\n",
      "        0.00154164,  0.00101143,  0.00213645,  0.00144064,  0.00060005,\n",
      "        0.00050263,  0.00066381,  0.00402431,  0.00150414]), 'param_alpha': masked_array(data = [0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4\n",
      " 0.4 0.4 0.4 0.4 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_l1_ratio': masked_array(data = [0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.0 0.1 0.2 0.3 0.4 0.5 0.6\n",
      " 0.7 0.8 0.9 1.0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.0 0.1 0.2\n",
      " 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'params': ({'alpha': 0.3, 'l1_ratio': 0.0}, {'alpha': 0.3, 'l1_ratio': 0.1}, {'alpha': 0.3, 'l1_ratio': 0.2}, {'alpha': 0.3, 'l1_ratio': 0.3}, {'alpha': 0.3, 'l1_ratio': 0.4}, {'alpha': 0.3, 'l1_ratio': 0.5}, {'alpha': 0.3, 'l1_ratio': 0.6}, {'alpha': 0.3, 'l1_ratio': 0.7}, {'alpha': 0.3, 'l1_ratio': 0.8}, {'alpha': 0.3, 'l1_ratio': 0.9}, {'alpha': 0.3, 'l1_ratio': 1.0}, {'alpha': 0.4, 'l1_ratio': 0.0}, {'alpha': 0.4, 'l1_ratio': 0.1}, {'alpha': 0.4, 'l1_ratio': 0.2}, {'alpha': 0.4, 'l1_ratio': 0.3}, {'alpha': 0.4, 'l1_ratio': 0.4}, {'alpha': 0.4, 'l1_ratio': 0.5}, {'alpha': 0.4, 'l1_ratio': 0.6}, {'alpha': 0.4, 'l1_ratio': 0.7}, {'alpha': 0.4, 'l1_ratio': 0.8}, {'alpha': 0.4, 'l1_ratio': 0.9}, {'alpha': 0.4, 'l1_ratio': 1.0}, {'alpha': 0.45, 'l1_ratio': 0.0}, {'alpha': 0.45, 'l1_ratio': 0.1}, {'alpha': 0.45, 'l1_ratio': 0.2}, {'alpha': 0.45, 'l1_ratio': 0.3}, {'alpha': 0.45, 'l1_ratio': 0.4}, {'alpha': 0.45, 'l1_ratio': 0.5}, {'alpha': 0.45, 'l1_ratio': 0.6}, {'alpha': 0.45, 'l1_ratio': 0.7}, {'alpha': 0.45, 'l1_ratio': 0.8}, {'alpha': 0.45, 'l1_ratio': 0.9}, {'alpha': 0.45, 'l1_ratio': 1.0}, {'alpha': 0.5, 'l1_ratio': 0.0}, {'alpha': 0.5, 'l1_ratio': 0.1}, {'alpha': 0.5, 'l1_ratio': 0.2}, {'alpha': 0.5, 'l1_ratio': 0.3}, {'alpha': 0.5, 'l1_ratio': 0.4}, {'alpha': 0.5, 'l1_ratio': 0.5}, {'alpha': 0.5, 'l1_ratio': 0.6}, {'alpha': 0.5, 'l1_ratio': 0.7}, {'alpha': 0.5, 'l1_ratio': 0.8}, {'alpha': 0.5, 'l1_ratio': 0.9}, {'alpha': 0.5, 'l1_ratio': 1.0})}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2441 (+/-0.1634) for {'alpha': 0.3, 'l1_ratio': 0.0}\n",
      "0.2373 (+/-0.1638) for {'alpha': 0.3, 'l1_ratio': 0.1}\n",
      "0.2335 (+/-0.1661) for {'alpha': 0.3, 'l1_ratio': 0.2}\n",
      "0.2322 (+/-0.1689) for {'alpha': 0.3, 'l1_ratio': 0.3}\n",
      "0.2324 (+/-0.1719) for {'alpha': 0.3, 'l1_ratio': 0.4}\n",
      "0.2337 (+/-0.1788) for {'alpha': 0.3, 'l1_ratio': 0.5}\n",
      "0.2364 (+/-0.1831) for {'alpha': 0.3, 'l1_ratio': 0.6}\n",
      "0.2399 (+/-0.1872) for {'alpha': 0.3, 'l1_ratio': 0.7}\n",
      "0.2435 (+/-0.2004) for {'alpha': 0.3, 'l1_ratio': 0.8}\n",
      "0.2453 (+/-0.2304) for {'alpha': 0.3, 'l1_ratio': 0.9}\n",
      "0.2378 (+/-0.2586) for {'alpha': 0.3, 'l1_ratio': 1.0}\n",
      "0.2455 (+/-0.1570) for {'alpha': 0.4, 'l1_ratio': 0.0}\n",
      "0.2385 (+/-0.1575) for {'alpha': 0.4, 'l1_ratio': 0.1}\n",
      "0.2351 (+/-0.1607) for {'alpha': 0.4, 'l1_ratio': 0.2}\n",
      "0.2348 (+/-0.1640) for {'alpha': 0.4, 'l1_ratio': 0.3}\n",
      "0.2356 (+/-0.1689) for {'alpha': 0.4, 'l1_ratio': 0.4}\n",
      "0.2371 (+/-0.1727) for {'alpha': 0.4, 'l1_ratio': 0.5}\n",
      "0.2397 (+/-0.1756) for {'alpha': 0.4, 'l1_ratio': 0.6}\n",
      "0.2438 (+/-0.1816) for {'alpha': 0.4, 'l1_ratio': 0.7}\n",
      "0.2488 (+/-0.1912) for {'alpha': 0.4, 'l1_ratio': 0.8}\n",
      "0.2535 (+/-0.2084) for {'alpha': 0.4, 'l1_ratio': 0.9}\n",
      "0.2526 (+/-0.2281) for {'alpha': 0.4, 'l1_ratio': 1.0}\n",
      "0.2450 (+/-0.1552) for {'alpha': 0.45, 'l1_ratio': 0.0}\n",
      "0.2380 (+/-0.1560) for {'alpha': 0.45, 'l1_ratio': 0.1}\n",
      "0.2344 (+/-0.1594) for {'alpha': 0.45, 'l1_ratio': 0.2}\n",
      "0.2346 (+/-0.1633) for {'alpha': 0.45, 'l1_ratio': 0.3}\n",
      "0.2355 (+/-0.1670) for {'alpha': 0.45, 'l1_ratio': 0.4}\n",
      "0.2369 (+/-0.1707) for {'alpha': 0.45, 'l1_ratio': 0.5}\n",
      "0.2398 (+/-0.1746) for {'alpha': 0.45, 'l1_ratio': 0.6}\n",
      "0.2432 (+/-0.1794) for {'alpha': 0.45, 'l1_ratio': 0.7}\n",
      "0.2484 (+/-0.1864) for {'alpha': 0.45, 'l1_ratio': 0.8}\n",
      "0.2535 (+/-0.2017) for {'alpha': 0.45, 'l1_ratio': 0.9}\n",
      "0.2552 (+/-0.2228) for {'alpha': 0.45, 'l1_ratio': 1.0}\n",
      "0.2442 (+/-0.1539) for {'alpha': 0.5, 'l1_ratio': 0.0}\n",
      "0.2370 (+/-0.1551) for {'alpha': 0.5, 'l1_ratio': 0.1}\n",
      "0.2332 (+/-0.1586) for {'alpha': 0.5, 'l1_ratio': 0.2}\n",
      "0.2334 (+/-0.1630) for {'alpha': 0.5, 'l1_ratio': 0.3}\n",
      "0.2344 (+/-0.1655) for {'alpha': 0.5, 'l1_ratio': 0.4}\n",
      "0.2356 (+/-0.1696) for {'alpha': 0.5, 'l1_ratio': 0.5}\n",
      "0.2385 (+/-0.1735) for {'alpha': 0.5, 'l1_ratio': 0.6}\n",
      "0.2414 (+/-0.1775) for {'alpha': 0.5, 'l1_ratio': 0.7}\n",
      "0.2468 (+/-0.1810) for {'alpha': 0.5, 'l1_ratio': 0.8}\n",
      "0.2521 (+/-0.1991) for {'alpha': 0.5, 'l1_ratio': 0.9}\n",
      "0.2557 (+/-0.2277) for {'alpha': 0.5, 'l1_ratio': 1.0}\n",
      "r2: 0.42862549450901444\n",
      "MSE: 224.93345309419337\n",
      "9.28 minutes elapsed\n",
      "9.28 minutes elapsed\n"
     ]
    }
   ],
   "source": [
    "scores = ['r2']\n",
    "# print(\"Running test {} of 3\".format(i+1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.9)\n",
    "# X_train, X_test, y_train, y_test = (X, X, Y, Y)\n",
    "t1 = time.time()\n",
    "for score in scores:\n",
    "    t3 = time.time()\n",
    "    params = [\n",
    "        {'alpha':[0.3, 0.4,0.45,  0.5], 'l1_ratio':[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "    ]\n",
    "    print(\"Tuning parameters for {}\\n\".format(score), flush = True)\n",
    "    reg = GridSearchCV(ElasticNet(), params, cv=10, scoring = score, n_jobs = 4, verbose = 3)\n",
    "    reg.fit(X_train, y_train)\n",
    "    print(reg)\n",
    "#     best_params.append(\"Best parameters: {}\".format(reg.best_params_))\n",
    "#     param_counts['C'][reg.best_params_[\"C\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     param_counts['gamma'][reg.best_params_[\"gamma\"]]+=1\n",
    "#     print(\"Best parameters:\\n\\n{}\".format(reg.best_params_), flush = True)\n",
    "    print(reg.cv_results_, flush = True)\n",
    "    means = reg.cv_results_['mean_test_score']\n",
    "    stds = reg.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, reg.cv_results_['params']):\n",
    "        print(\"%0.4f (+/-%0.04f) for %r\" % (mean, std * 2, params), flush = True)\n",
    "\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(\"r2: {}\".format(r2_score(y_test, y_pred)))\n",
    "    print(\"MSE: {}\".format(mean_squared_error(y_test, y_pred)))\n",
    "    t4 = time.time()\n",
    "    print(\"%0.2f minutes elapsed\" %((t4-t3)/60), flush = True)\n",
    "#     r2s.append(r2_score(y_test, y_pred))\n",
    "#     mse.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"%0.2f minutes elapsed\" %((t2-t1)/60), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2799 (+/-0.2634) for {'alpha': 0.4, 'l1_ratio': 1.0}\n",
      "0.2759 (+/-0.1873) for {'alpha': 0.3, 'l1_ratio': 0.0}\n",
      "0.2753 (+/-0.1868) for {'alpha': 0.4, 'l1_ratio': 0.0}\n",
      "0.2724 (+/-0.2436) for {'alpha': 0.4, 'l1_ratio': 0.9}\n",
      "0.2679 (+/-0.1983) for {'alpha': 0.3, 'l1_ratio': 0.1}\n",
      "0.2658 (+/-0.1979) for {'alpha': 0.4, 'l1_ratio': 0.1}\n",
      "0.2651 (+/-0.2302) for {'alpha': 0.4, 'l1_ratio': 0.8}\n",
      "0.2638 (+/-0.2486) for {'alpha': 0.3, 'l1_ratio': 0.9}\n",
      "0.2635 (+/-0.2547) for {'alpha': 0.3, 'l1_ratio': 1.0}\n",
      "0.2621 (+/-0.2063) for {'alpha': 0.3, 'l1_ratio': 0.2}\n",
      "0.2610 (+/-0.2340) for {'alpha': 0.3, 'l1_ratio': 0.8}\n",
      "0.2597 (+/-0.2058) for {'alpha': 0.4, 'l1_ratio': 0.2}\n",
      "0.2589 (+/-0.2243) for {'alpha': 0.4, 'l1_ratio': 0.7}\n",
      "0.2582 (+/-0.2115) for {'alpha': 0.3, 'l1_ratio': 0.3}\n",
      "0.2568 (+/-0.2100) for {'alpha': 0.4, 'l1_ratio': 0.3}\n",
      "0.2567 (+/-0.2174) for {'alpha': 0.3, 'l1_ratio': 0.6}\n",
      "0.2567 (+/-0.2130) for {'alpha': 0.3, 'l1_ratio': 0.4}\n",
      "0.2566 (+/-0.2244) for {'alpha': 0.3, 'l1_ratio': 0.7}\n",
      "0.2561 (+/-0.2145) for {'alpha': 0.3, 'l1_ratio': 0.5}\n",
      "0.2555 (+/-0.2158) for {'alpha': 0.4, 'l1_ratio': 0.5}\n",
      "0.2555 (+/-0.2198) for {'alpha': 0.4, 'l1_ratio': 0.6}\n",
      "0.2555 (+/-0.2134) for {'alpha': 0.4, 'l1_ratio': 0.4}\n"
     ]
    }
   ],
   "source": [
    "sort_means = means\n",
    "sort_stds = stds\n",
    "sort_params = reg.cv_results_['params']\n",
    "sort_means, sort_stds, sort_params = zip(*sorted(zip(sort_means, sort_stds, sort_params), key = lambda x:x[0], reverse = True))\n",
    "for mean, std, params in zip(sort_means, sort_stds, sort_params):\n",
    "        print(\"%0.4f (+/-%0.04f) for %r\" % (mean, std * 2, params), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612603598534\n",
      "0.556859757989\n",
      "0.583152959299\n",
      "0.526456397914\n",
      "0.598533653587\n",
      "0.530089205469\n",
      "0.60267488958\n",
      "0.557338721115\n",
      "0.552213862592\n",
      "0.505163134789\n",
      "0.605043676849\n",
      "0.63564552184\n",
      "0.655256844186\n",
      "0.530869824313\n",
      "0.540638431792\n",
      "0.546789289465\n",
      "0.596192505272\n",
      "0.536598280399\n",
      "0.538086282622\n",
      "0.547774067462\n",
      "0.552247354468\n",
      "0.525203665478\n",
      "0.497885450003\n",
      "0.571677598537\n",
      "0.551397518467\n",
      "0.5066218316\n",
      "0.506093841825\n",
      "0.472085218748\n",
      "0.575718155977\n",
      "0.474905943708\n",
      "0.556948460114\n",
      "0.523984833844\n",
      "0.582999716345\n",
      "0.601442795349\n",
      "0.519884906527\n",
      "0.630954274109\n",
      "0.476698511038\n",
      "0.558846936138\n",
      "0.54638147667\n",
      "0.48282937803\n",
      "0.598089600685\n",
      "0.478378259879\n",
      "0.445883692589\n",
      "0.524703237621\n",
      "0.474520761355\n",
      "0.547509492474\n",
      "0.462418525266\n",
      "0.489947377301\n",
      "0.576346344618\n",
      "0.514306157492\n",
      "Mean: 0.5436978444264973\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.8)\n",
    "    elasticnet_model = ElasticNet(alpha = 0.4, l1_ratio = 1.0).fit(np.ascontiguousarray(X), Y)\n",
    "    pred = elasticnet_model.predict(X_test)\n",
    "    print(r2_score(y_test, pred))\n",
    "    scores.append(r2_score(y_test, pred))\n",
    "print(\"Mean: {}\".format(np.mean(np.asarray(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0., ...,  0.,  0., -0.])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticnet_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.479741250061009"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticnet_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet = elasticnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.41045326,  -0.73560948,   1.33472758,   1.51154873,\n",
       "         0.17157215,   0.98691794,  -0.21274063,   0.91972967,\n",
       "         2.5871713 ,  -1.39638219,  -2.84962092,   0.85910996,\n",
       "         2.065028  ,   0.95766534,   1.119264  ,  -0.06832318,\n",
       "         1.36966053,   0.48727577,   0.40620614,   0.96600176,\n",
       "         2.13582015,  -5.1643358 ,   0.53418616,   0.89251387,\n",
       "        -0.26845376,  -6.04398754,  -0.22038223,  -2.23406735,\n",
       "        -0.51509606,  -0.88449428,   0.83239166,   0.87662056,\n",
       "         0.04782496,  -0.53894968,   0.14222311,   0.41591191,\n",
       "         0.64680358,  -2.64390543,  -2.18071383,   0.12786842,\n",
       "         0.79121765,  -0.18350004,   0.90928303,   2.18153614,\n",
       "        -0.45855552,  -0.30622616,   3.18190458,  -0.71018951,\n",
       "         1.24517896,   0.0968022 ,  -0.21593965,   0.7536336 ,\n",
       "         0.11340514,   0.42418559,   2.68236133,   2.46313209,\n",
       "        -0.59899694,  -0.06561387,   0.53549384,   0.45923755,\n",
       "         0.59427256,   0.16053104,  -0.26685905,  -0.77835725,\n",
       "        -0.63007056,  -0.38194278,  -0.87054823,  -1.51328421,\n",
       "        -1.32327706,   0.24193025,  -1.50185249,  -0.37095991,\n",
       "         0.07476485,  -0.61751587,   0.22140864,  -0.25874661,\n",
       "        -5.68629687, -10.54965706,  -0.71167382,   0.47836539,\n",
       "         1.41134903,   0.26712346,   1.15668409])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.coef_[enet.coef_!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17,   30,  143,  175,  336,  436,  530,  543,  665,  695,  786,\n",
       "        804,  885, 1114, 1119, 1123, 1182, 1201, 1343, 1817, 1843, 1891,\n",
       "       1951, 2007, 2029, 2106, 2215, 2242, 2245, 2326, 2496, 2588, 2622,\n",
       "       2626, 2635, 2671, 2748, 2813, 2877, 2935, 2972, 3315, 3350, 3395,\n",
       "       3425, 3448, 3682, 3810, 4012, 4027, 4058, 4063, 4098, 4139, 4210,\n",
       "       4241, 4294, 4312, 4616, 4912, 4962, 5201, 5349, 5359, 5387, 5453,\n",
       "       5470, 5540, 5542, 5557, 5562, 5564, 5576, 5579, 5580, 5581, 5583,\n",
       "       5584, 5590, 5598, 5601, 5614, 5628], dtype=int64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(enet.coef_!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Either fit the model before transform or set \"prefit=True\" while passing the fitted estimator to the constructor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-ebae47abac32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ])\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0menet_inds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\base.py\u001b[0m in \u001b[0;36mget_support\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mindices\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_support_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sky\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\from_model.py\u001b[0m in \u001b[0;36m_get_support_mask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             raise ValueError(\n\u001b[1;32m--> 198\u001b[1;33m                 \u001b[1;34m'Either fit the model before transform or set \"prefit=True\"'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m                 ' while passing the fitted estimator to the constructor.')\n\u001b[0;32m    200\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_feature_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Either fit the model before transform or set \"prefit=True\" while passing the fitted estimator to the constructor."
     ]
    }
   ],
   "source": [
    "enet = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(ElasticNet(alpha = 0.4, l1_ratio = 1.0))),\n",
    "    ('regression', ElasticNet(alpha = 0.4, l1_ratio = 1.0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51430352769103094"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.fit(X, Y)\n",
    "pred = enet.predict(X_test)\n",
    "sklearn.metrics.r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet_inds = enet.steps[0][1].get_support(indices = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17,   30,  143,  175,  336,  436,  530,  543,  665,  695,  786,\n",
       "        804,  885, 1114, 1119, 1123, 1182, 1201, 1343, 1817, 1843, 1891,\n",
       "       1951, 2007, 2029, 2106, 2215, 2242, 2245, 2326, 2496, 2588, 2622,\n",
       "       2626, 2635, 2671, 2748, 2813, 2877, 2935, 2972, 3315, 3350, 3395,\n",
       "       3425, 3448, 3682, 3810, 4012, 4027, 4058, 4063, 4098, 4139, 4210,\n",
       "       4241, 4294, 4312, 4616, 4912, 4962, 5201, 5349, 5359, 5387, 5453,\n",
       "       5470, 5540, 5542, 5557, 5562, 5564, 5576, 5579, 5580, 5581, 5583,\n",
       "       5584, 5590, 5598, 5601, 5614, 5628], dtype=int64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pos4_R', 'Pos6_I', 'Pos19_I', 'Pos23_W', 'Pos59_K', 'Pos84_M',\n",
       "       'Pos102_E', 'Pos106_E', 'Pos134_A', 'Pos135_T', 'Pos147_-',\n",
       "       'Pos147_D', 'Pos151_K', 'Pos172_E', 'Pos173_Y', 'Pos173_Q',\n",
       "       'Pos181_I', 'Pos183_P', 'Pos198_T', 'Pos291_S', 'Pos293_E',\n",
       "       'Pos300_N', 'Pos306_R', 'Pos315_Q', 'Pos319_A', 'Pos325_D',\n",
       "       'Pos337_E', 'Pos340_N', 'Pos340_R', 'Pos347_E', 'Pos363_P',\n",
       "       'Pos379_G', 'Pos388_T', 'Pos389_G', 'Pos389_Q', 'Pos393_S',\n",
       "       'Pos398_S', 'Pos411_E', 'Pos416_I', 'Pos429_G', 'Pos434_M',\n",
       "       'Pos492_K', 'Pos500_K', 'Pos507_E', 'Pos514_V', 'Pos518_V',\n",
       "       'Pos580_I', 'Pos607_N', 'Pos633_K', 'Pos636_S', 'Pos640_N',\n",
       "       'Pos640_D', 'Pos644_N', 'Pos648_E', 'Pos659_D', 'Pos665_K',\n",
       "       'Pos674_D', 'Pos677_K', 'Pos732_G', 'Pos777_I', 'Pos783_E',\n",
       "       'Pos812_I', 'Pos832_L', 'Pos833_I', 'Pos836_L', 'Pos843_I',\n",
       "       'Pos845_R', 'N137', 'N139', 'N147', 'N185', 'N186_1', 'N234',\n",
       "       'N276', 'N289', 'N293', 'N301', 'N332', 'N354_1', 'N392', 'N397',\n",
       "       'N410', 'N465'], dtype=object)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[enet_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"enet.pkl\", 'wb') as f:\n",
    "    pkl.dump(enet_inds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
